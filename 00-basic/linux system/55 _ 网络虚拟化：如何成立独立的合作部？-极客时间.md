上一节，我们讲了存储虚拟化，这一节我们来讲网络虚拟化。

网络虚拟化有和存储虚拟化类似的地方，例如，它们都是基于 virtio 的，因而我们在看网络虚拟化的过程中，会看到和存储虚拟化很像的数据结构和原理。但是，网络虚拟化也有自己的特殊性。例如，存储虚拟化是将宿主机上的文件作为客户机上的硬盘，而网络虚拟化需要依赖于内核协议栈进行网络包的封装与解封装。那怎么实现客户机和宿主机之间的互通呢？我们就一起来看一看。

## 解析初始化过程

我们还是从 Virtio Network Device 这个设备的初始化讲起。

static 

 const TypeInfo device\_type\_info = {

.name = TYPE_DEVICE,

.parent = TYPE_OBJECT,

.instance_size = sizeof(DeviceState),

.instance\_init = device\_initfn,

.instance\_post\_init = device\_post\_init,

.instance\_finalize = device\_finalize,

.class\_base\_init = device\_class\_base_init,

.class\_init = device\_class_init,

.abstract = true,

.class_size = sizeof(DeviceClass),

};

static 

 const TypeInfo virtio\_device\_info = {

.name = TYPE\_VIRTIO\_DEVICE,

.parent = TYPE_DEVICE,

.instance_size = sizeof(VirtIODevice),

.class\_init = virtio\_device\_class\_init,

.instance\_finalize = virtio\_device\_instance\_finalize,

.abstract = true,

.class_size = sizeof(VirtioDeviceClass),

};

static 

 const TypeInfo virtio\_net\_info = {

.name = TYPE\_VIRTIO\_NET,

.parent = TYPE\_VIRTIO\_DEVICE,

.instance_size = sizeof(VirtIONet),

.instance\_init = virtio\_net\_instance\_init,

.class\_init = virtio\_net\_class\_init,

};

static 

 void virtio\_register\_types(void)

{

type\_register\_static(&virtio\_net\_info);

}

type\_init(virtio\_register_types)

Virtio Network Device 这种类的定义是有多层继承关系的，TYPE\_VIRTIO\_NET 的父类是 TYPE\_VIRTIO\_DEVICE，TYPE\_VIRTIO\_DEVICE 的父类是 TYPE\_DEVICE，TYPE\_DEVICE 的父类是 TYPE_OBJECT，继承关系到头了。

type\_init 用于注册这种类。这里面每一层都有 class\_init，用于从 TypeImpl 生成 xxxClass，也有 instance_init，会将 xxxClass 初始化为实例。

TYPE\_VIRTIO\_NET 层的 class\_init 函数 virtio\_net\_class\_init，定义了 DeviceClass 的 realize 函数为 virtio\_net\_device_realize，这一点和存储块设备是一样的。

static void virtio\_net\_device_realize(DeviceState \*dev, Error \*\*errp)

{

VirtIODevice *vdev = VIRTIO_DEVICE(dev);

VirtIONet *n = VIRTIO_NET(dev);

NetClientState *nc;

int i;

......

virtio_init(vdev, "virtio-net", VIRTIO\_ID\_NET, n->config_size);

\* We set a lower limit on RX queue size to what it always was.

\* Guests that want a smaller ring can always resize it without

\* help from us (using virtio 1 and up).

*/

if (n->net\_conf.rx\_queue\_size < VIRTIO\_NET\_RX\_QUEUE\_MIN\_SIZE ||

n->net\_conf.rx\_queue\_size > VIRTQUEUE\_MAX_SIZE ||

!is\_power\_of_2(n->net\_conf.rx\_queue_size)) {

......

return;

}

if (n->net\_conf.tx\_queue\_size < VIRTIO\_NET\_TX\_QUEUE\_MIN\_SIZE ||

n->net\_conf.tx\_queue\_size > VIRTQUEUE\_MAX_SIZE ||

!is\_power\_of_2(n->net\_conf.tx\_queue_size)) {

......

return;

}

n->max_queues = MAX(n->nic_conf.peers.queues, 1);

if (n->max_queues * 2 \+ 1 \> VIRTIO\_QUEUE\_MAX) {

......

return;

}

n->vqs = g_malloc0(sizeof(VirtIONetQueue) * n->max_queues);

n->curr_queues = 1;

......

n->net\_conf.tx\_queue_size = MIN(virtio\_net\_max\_tx\_queue_size(n),

n->net\_conf.tx\_queue_size);

for (i = 0; i < n->max_queues; i++) {

virtio\_net\_add_queue(n, i);

}

n->ctrl_vq = virtio\_add\_queue(vdev, 64, virtio\_net\_handle_ctrl);

qemu\_macaddr\_default\_if\_unset(&n->nic_conf.macaddr);

memcpy(&n->mac\[0\], &n->nic_conf.macaddr, sizeof(n->mac));

n->status = VIRTIO\_NET\_S\_LINK\_UP;

if (n->netclient_type) {

n->nic = qemu\_new\_nic(&net\_virtio\_info, &n->nic_conf,

n->netclient_type, n->netclient_name, n);

} else {

n->nic = qemu\_new\_nic(&net\_virtio\_info, &n->nic_conf,

object\_get\_typename(OBJECT(dev)), dev->id, n);

}

......

}

这里面创建了一个 VirtIODevice，这一点和存储虚拟化也是一样的。virtio\_init 用来初始化这个设备。VirtIODevice 结构里面有一个 VirtQueue 数组，这就是 virtio 前端和后端互相传数据的队列，最多有 VIRTIO\_QUEUE_MAX 个。

刚才我们说的都是一样的地方，其实也有不一样的地方，我们下面来看。

你会发现，这里面有这样的语句 n->max\_queues * 2 + 1 > VIRTIO\_QUEUE_MAX。为什么要乘以 2 呢？这是因为，对于网络设备来讲，应该分发送队列和接收队列两个方向，所以乘以 2。

接下来，我们调用 virtio\_net\_add\_queue 来初始化队列，可以看出来，这里面就有发送 tx\_vq 和接收 rx_vq 两个队列。

typedef struct 

 VirtIONetQueue {

VirtQueue *rx_vq;

VirtQueue *tx_vq;

QEMUTimer *tx_timer;

QEMUBH *tx_bh;

uint32\_t tx\_waiting;

struct {

VirtQueueElement *elem;

} async_tx;

struct 

 VirtIONet *n;

} VirtIONetQueue;

static void virtio\_net\_add_queue(VirtIONet *n, int index)

{

VirtIODevice *vdev = VIRTIO_DEVICE(n);

n->vqs\[index\].rx_vq = virtio\_add\_queue(vdev, n->net\_conf.rx\_queue\_size, virtio\_net\_handle\_rx);

......

n->vqs\[index\].tx_vq = virtio\_add\_queue(vdev, n->net\_conf.tx\_queue\_size, virtio\_net\_handle\_tx_bh);

n->vqs\[index\].tx_bh = qemu\_bh\_new(virtio\_net\_tx_bh, &n->vqs\[index\]);

n->vqs\[index\].n = n;

}

每个 VirtQueue 中，都有一个 vring 用来维护这个队列里面的数据；另外还有函数 virtio\_net\_handle\_rx 用于处理网络包的接收；函数 virtio\_net\_handle\_tx_bh 用于网络包的发送，这个函数我们后面会用到。

NICState *qemu\_new\_nic(NetClientInfo *info,

NICConf *conf,

const 

 char *model,

const 

 char *name,

void *opaque)

{

NetClientState **peers = conf->peers.ncs;

NICState *nic;

int i, queues = MAX(1, conf->peers.queues);

......

nic = g_malloc0(info->size + sizeof(NetClientState) * queues);

nic->ncs = (void *)nic + info->size;

nic->conf = conf;

nic->opaque = opaque;

for (i = 0; i < queues; i++) {

qemu\_net\_client_setup(&nic->ncs\[i\], info, peers\[i\], model, name, NULL);

nic->ncs\[i\].queue_index = i;

}

return nic;

}

static void qemu\_net\_client_setup(NetClientState *nc,

NetClientInfo *info,

NetClientState *peer,

const 

 char *model,

const 

 char *name,

NetClientDestructor *destructor)

{

nc->info = info;

nc->model = g_strdup(model);

if (name) {

nc->name = g_strdup(name);

} else {

nc->name = assign_name(nc, model);

}

QTAILQ\_INSERT\_TAIL(&net_clients, nc, next);

nc->incoming_queue = qemu\_new\_net_queue(qemu\_deliver\_packet_iov, nc);

nc->destructor = destructor;

QTAILQ_INIT(&nc->filters);

}

接下来，qemu\_new\_nic 会创建一个虚拟机里面的网卡。

## qemu 的启动过程中的网络虚拟化

初始化过程解析完毕以后，我们接下来从 qemu 的启动过程看起。

对于网卡的虚拟化，qemu 的启动参数里面有关的是下面两行：

-netdev tap,fd=32,id=hostnet0,vhost=on,vhostfd=37

-device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:d1:2d:99,bus=pci.0,addr=0x3

qemu 的 main 函数会调用 net\_init\_clients 进行网络设备的初始化，可以解析 net 参数，也可以在 net\_init\_clients 中解析 netdev 参数。

int 

 net\_init\_clients(Error **errp)

{

QTAILQ_INIT(&net_clients);

if (qemu\_opts\_foreach(qemu\_find\_opts("netdev"),

net\_init\_netdev, NULL, errp)) {

return 

 -1;

}

if (qemu\_opts\_foreach(qemu\_find\_opts("nic"), net\_param\_nic, NULL, errp)) {

return 

 -1;

}

if (qemu\_opts\_foreach(qemu\_find\_opts("net"), net\_init\_client, NULL, errp)) {

return 

 -1;

}

return 

 0;

}

net\_init\_clients 会解析参数。上面的参数 netdev 会调用 net\_init\_netdev->net\_client\_init->net\_client\_init1。

net\_client\_init1 会根据不同的 driver 类型，调用不同的初始化函数。

static 

 int 

 (\* const net\_client\_init\_fun\[NET\_CLIENT\_DRIVER\_\_MAX\])(

const Netdev *netdev,

const 

 char *name,

NetClientState \*peer, Error \*\*errp) = {

\[NET\_CLIENT\_DRIVER\_NIC\] = net\_init_nic,

\[NET\_CLIENT\_DRIVER\_TAP\] = net\_init_tap,

\[NET\_CLIENT\_DRIVER\_SOCKET\] = net\_init_socket,

\[NET\_CLIENT\_DRIVER\_HUBPORT\] = net\_init_hubport,

......

};

由于我们配置的 driver 的类型是 tap，因而这里会调用 net\_init\_tap->net\_tap\_init->tap_open。

#define PATH\_NET\_TUN "/dev/net/tun"

int 

 tap_open(char *ifname, int ifname_size, int *vnet_hdr,

int vnet\_hdr\_required, int mq_required, Error **errp)

{

struct 

 ifreq ifr;

int fd, ret;

int len = sizeof(struct virtio\_net\_hdr);

unsigned 

 int features;

TFR(fd = open(PATH\_NET\_TUN, O_RDWR));

memset(&ifr, 0, sizeof(ifr));

ifr.ifr\_flags = IFF\_TAP | IFF\_NO\_PI;

if (ioctl(fd, TUNGETFEATURES, &features) == -1) {

features = 0;

}

if (features & IFF\_ONE\_QUEUE) {

ifr.ifr\_flags |= IFF\_ONE_QUEUE;

}

if (*vnet_hdr) {

if (features & IFF\_VNET\_HDR) {

*vnet_hdr = 1;

ifr.ifr\_flags |= IFF\_VNET_HDR;

} else {

*vnet_hdr = 0;

}

ioctl(fd, TUNSETVNETHDRSZ, &len);

}

......

ret = ioctl(fd, TUNSETIFF, (void *) &ifr);

......

fcntl(fd, F\_SETFL, O\_NONBLOCK);

return fd;

}

在 tap_open 中，我们打开一个文件"/dev/net/tun"，然后通过 ioctl 操作这个文件。这是 Linux 内核的一项机制，和 KVM 机制很像。其实这就是一种通过打开这个字符设备文件，然后通过 ioctl 操作这个文件和内核打交道，来使用内核的能力。

![[243e93913b18c3ab00be5676bef334d3_ec12344f77364be7a.png]]

为什么需要使用内核的机制呢？因为网络包需要从虚拟机里面发送到虚拟机外面，发送到宿主机上的时候，必须是一个正常的网络包才能被转发。要形成一个网络包，我们那就需要经过复杂的协议栈，协议栈的复杂咱们在发送网络包那一节讲过了。

客户机会将网络包发送给 qemu。qemu 自己没有网络协议栈，现去实现一个也不可能，太复杂了。于是，它就要借助内核的力量。

qemu 会将客户机发送给它的网络包，然后转换成为文件流，写入"/dev/net/tun"字符设备。就像写一个文件一样。内核中 TUN/TAP 字符设备驱动会收到这个写入的文件流，然后交给 TUN/TAP 的虚拟网卡驱动。这个驱动会将文件流再次转成网络包，交给 TCP/IP 栈，最终从虚拟 TAP 网卡 tap0 发出来，成为标准的网络包。后面我们会看到这个过程。

现在我们到内核里面，看一看打开"/dev/net/tun"字符设备后，内核会发生什么事情。内核的实现在 drivers/net/tun.c 文件中。这是一个字符设备驱动程序，应该符合字符设备的格式。

module_init(tun_init);

module_exit(tun_cleanup);

MODULE_DESCRIPTION(DRV_DESCRIPTION);

MODULE_AUTHOR(DRV_COPYRIGHT);

MODULE_LICENSE("GPL");

MODULE\_ALIAS\_MISCDEV(TUN_MINOR);

MODULE_ALIAS("devname:net/tun");

static int __init tun_init(void)

{

......

ret = rtnl\_link\_register(&tun\_link\_ops);

......

ret = misc_register(&tun_miscdev);

......

ret = register\_netdevice\_notifier(&tun\_notifier\_block);

......

}

这里面注册了一个 tun_miscdev 字符设备，从它的定义可以看出，这就是"/dev/net/tun"字符设备。

static 

 struct 

 miscdevice tun_miscdev = {

.minor = TUN_MINOR,

.name = "tun",

.nodename = "net/tun",

.fops = &tun_fops,

};

static 

 const 

 struct 

 file_operations tun_fops = {

.owner = THIS_MODULE,

.llseek = no_llseek,

.read\_iter = tun\_chr\_read\_iter,

.write\_iter = tun\_chr\_write\_iter,

.poll = tun\_chr\_poll,

.unlocked\_ioctl = tun\_chr_ioctl,

.open = tun\_chr\_open,

.release = tun\_chr\_close,

.fasync = tun\_chr\_fasync,

};

qemu 的 tap\_open 函数会打开这个字符设备 PATH\_NET\_TUN。打开字符设备的过程我们不再重复。我就说一下，到了驱动这一层，调用的是 tun\_chr_open。

static int tun\_chr\_open(struct 

 inode *inode, struct 

 file \* file)

{

struct 

 tun_file *tfile;

tfile = (struct 

 tun_file *)sk_alloc(net, AF\_UNSPEC, GFP\_KERNEL,

&tun_proto, 0);

RCU\_INIT\_POINTER(tfile->tun, NULL);

tfile->flags = 0;

tfile->ifindex = 0;

init\_waitqueue\_head(&tfile->wq.wait);

RCU\_INIT\_POINTER(tfile->socket.wq, &tfile->wq);

tfile->socket.file = file;

tfile->socket.ops = &tun\_socket\_ops;

sock\_init\_data(&tfile->socket, &tfile->sk);

tfile->sk.sk\_write\_space = tun\_sock\_write_space;

tfile->sk.sk\_sndbuf = INT\_MAX;

file->private_data = tfile;

INIT\_LIST\_HEAD(&tfile->next);

sock\_set\_flag(&tfile->sk, SOCK_ZEROCOPY);

return 

 0;

}

在 tun\_chr\_open 的参数里面，有一个 struct file，这是代表什么文件呢？它代表的就是打开的字符设备文件"/dev/net/tun"，因而往这个字符设备文件中写数据，就会通过这个 struct file 写入。这个 struct file 里面的 file\_operations，按照字符设备打开的规则，指向的就是 tun\_fops。

另外，我们还需要在 tun\_chr\_open 创建了一个结构 struct tun\_file，并且将 struct file 的 private\_data 指向它。

\* also contains all socket related structures

\* to serve as one transmit queue for tuntap device.

*/

struct 

 tun_file {

struct 

 sock sk;

struct 

 socket socket;

struct 

 socket_wq wq;

struct 

 tun_struct __rcu *tun;

struct 

 fasync_struct *fasync;

unsigned 

 int flags;

union {

u16 queue_index;

unsigned 

 int ifindex;

};

struct 

 list_head next;

struct 

 tun_struct *detached;

struct 

 skb_array tx_array;

};

struct 

 tun_struct {

struct 

 tun_file \_\_rcu *tfiles\[MAX\_TAP_QUEUES\];

unsigned 

 int numqueues;

unsigned 

 int flags;

kuid_t owner;

kgid_t group;

struct 

 net_device *dev;

netdev\_features\_t set_features;

int align;

int vnet\_hdr\_sz;

int sndbuf;

struct 

 tap_filter txflt;

struct 

 sock_fprog fprog;

bool filter_attached;

spinlock_t lock;

struct 

 hlist_head flows\[TUN\_NUM\_FLOW_ENTRIES\];

struct 

 timer_list flow\_gc\_timer;

unsigned 

 long ageing_time;

unsigned 

 int numdisabled;

struct 

 list_head disabled;

void *security;

u32 flow_count;

u32 rx_batched;

struct 

 tun\_pcpu\_stats \_\_percpu *pcpu\_stats;

};

static 

 const 

 struct 

 proto_ops tun\_socket\_ops = {

.peek\_len = tun\_peek_len,

.sendmsg = tun_sendmsg,

.recvmsg = tun_recvmsg,

};

在 struct tun\_file 中，有一个成员 struct tun\_struct，它里面有一个 struct net\_device，这个用来表示宿主机上的 tuntap 网络设备。在 struct tun\_file 中，还有 struct socket 和 struct sock，因为要用到内核的网络协议栈，所以就需要这两个结构，这在网络协议那一节已经分析过了。

所以，按照 struct tun\_file 的注释说的，这是一个很重要的数据结构。"/dev/net/tun"对应的 struct file 的 private\_data 指向它，因而可以接收 qemu 发过来的数据。除此之外，它还可以通过 struct sock 来操作内核协议栈，然后将网络包从宿主机上的 tuntap 网络设备发出去，宿主机上的 tuntap 网络设备对应的 struct net_device 也归它管。

在 qemu 的 tap_open 函数中，打开这个字符设备文件之后，接下来要做的事情是，通过 ioctl 来设置宿主机的网卡 TUNSETIFF。

接下来，ioctl 到了内核里面，会调用 tun\_chr\_ioctl。

static 

 long \_\_tun\_chr_ioctl(struct file *file, unsigned 

 int cmd,

unsigned 

 long arg, int ifreq_len)

{

struct 

 tun_file *tfile = file->private_data;

struct 

 tun_struct *tun;

void __user* argp = (void __user*)arg;

struct 

 ifreq ifr;

kuid_t owner;

kgid_t group;

int sndbuf;

int vnet\_hdr\_sz;

unsigned 

 int ifindex;

int le;

int ret;

if (cmd == TUNSETIFF || cmd == TUNSETQUEUE || \_IOC\_TYPE(cmd) == SOCK\_IOC\_TYPE) {

if (copy\_from\_user(&ifr, argp, ifreq_len))

return -EFAULT;

}

......

tun = \_\_tun\_get(tfile);

if (cmd == TUNSETIFF) {

ifr.ifr_name\[IFNAMSIZ-1\] = '\\0';

ret = tun\_set\_iff(sock_net(&tfile->sk), file, &ifr);

......

if (copy\_to\_user(argp, &ifr, ifreq_len))

ret = -EFAULT;

}

......

}

在 \_\_tun\_chr\_ioctl 中，我们首先通过 copy\_from\_user 把配置从用户态拷贝到内核态，调用 tun\_set\_iff 设置 tuntap 网络设备，然后调用 copy\_to_user 将配置结果返回。

static int tun\_set\_iff(struct 

 net *net, struct 

 file *file, struct 

 ifreq *ifr)

{

struct 

 tun_struct *tun;

struct 

 tun_file *tfile = file->private_data;

struct 

 net_device *dev;

......

char *name;

unsigned long flags = 0;

int queues = ifr->ifr\_flags & IFF\_MULTI_QUEUE ?

MAX\_TAP\_QUEUES : 1;

if (ifr->ifr\_flags & IFF\_TUN) {

flags |= IFF_TUN;

name = "tun%d";

} else 

 if (ifr->ifr\_flags & IFF\_TAP) {

flags |= IFF_TAP;

name = "tap%d";

} else

return -EINVAL;

if (*ifr->ifr_name)

name = ifr->ifr_name;

dev = alloc\_netdev\_mqs(sizeof(struct 

 tun_struct), name,

NET\_NAME\_UNKNOWN, tun_setup, queues,

queues);

err = dev\_get\_valid_name(net, dev, name);

dev\_net\_set(dev, net);

dev->rtnl\_link\_ops = &tun\_link\_ops;

dev->ifindex = tfile->ifindex;

dev->sysfs_groups\[0\] = &tun\_attr\_group;

tun = netdev_priv(dev);

tun->dev = dev;

tun->flags = flags;

tun->txflt.count = 0;

tun->vnet\_hdr\_sz = sizeof(struct 

 virtio\_net\_hdr);

tun->align = NET\_SKB\_PAD;

tun->filter_attached = false;

tun->sndbuf = tfile->socket.sk->sk_sndbuf;

tun->rx_batched = 0;

tun\_net\_init(dev);

tun\_flow\_init(tun);

err = tun_attach(tun, file, false);

err = register_netdevice(tun->dev);

netif\_carrier\_on(tun->dev);

if (netif_running(tun->dev))

netif\_tx\_wake\_all\_queues(tun->dev);

strcpy(ifr->ifr_name, tun->dev->name);

return 

 0;

}

tun\_set\_iff 创建了 struct tun\_struct 和 struct net\_device，并且将这个 tuntap 网络设备通过 register_netdevice 注册到内核中。这样，我们就能在宿主机上通过 ip addr 看到这个网卡了。

![[9826223c7375bec19bd13588f3875ffd_fa6d0901062048829.png]]

至此宿主机上的内核的数据结构也完成了。

## 关联前端设备驱动和后端设备驱动

下面，我们来解析在客户机中发送一个网络包的时候，会发生哪些事情。

虚拟机里面的进程发送一个网络包，通过文件系统和 Socket 调用网络协议栈，到达网络设备层。只不过这个不是普通的网络设备，而是 virtio_net 的驱动。

virtio\_net 的驱动程序代码在 Linux 操作系统的源代码里面，文件名为 drivers/net/virtio\_net.c。

static __init int 

 virtio\_net\_driver_init(void)

{

ret = register\_virtio\_driver(&virtio\_net\_driver);

......

}

module\_init(virtio\_net\_driver\_init);

module\_exit(virtio\_net\_driver\_exit);

MODULE\_DEVICE\_TABLE(virtio, id_table);

MODULE_DESCRIPTION("Virtio network driver");

MODULE_LICENSE("GPL");

static struct virtio_driver 

 virtio\_net\_driver 

 = {

.driver.name = KBUILD_MODNAME,

.driver.owner = THIS_MODULE,

.id\_table = id\_table,

.validate = virtnet_validate,

.probe = virtnet_probe,

.remove = virtnet_remove,

.config\_changed = virtnet\_config_changed,

......

};

在 virtio\_net 的驱动程序的初始化代码中，我们需要注册一个驱动函数 virtio\_net_driver。

当一个设备驱动作为一个内核模块被初始化的时候，probe 函数会被调用，因而我们来看一下 virtnet_probe。

static int virtnet_probe(struct 

 virtio_device *vdev)

{

int i, err;

struct 

 net_device *dev;

struct 

 virtnet_info *vi;

u16 max\_queue\_pairs;

int mtu;

dev = alloc\_etherdev\_mq(sizeof(struct 

 virtnet_info), max\_queue\_pairs);

dev->priv\_flags |= IFF\_UNICAST\_FLT | IFF\_LIVE\_ADDR\_CHANGE;

dev->netdev\_ops = &virtnet\_netdev;

dev->features = NETIF\_F\_HIGHDMA;

dev->ethtool\_ops = &virtnet\_ethtool_ops;

SET\_NETDEV\_DEV(dev, &vdev->dev);

......

dev->min\_mtu = MIN\_MTU;

dev->max\_mtu = MAX\_MTU;

vi = netdev_priv(dev);

vi->dev = dev;

vi->vdev = vdev;

vdev->priv = vi;

vi->stats = alloc_percpu(struct 

 virtnet_stats);

INIT_WORK(&vi->config\_work, virtnet\_config\_changed\_work);

......

vi->max\_queue\_pairs = max\_queue\_pairs;

err = init_vqs(vi);

netif\_set\_real\_num\_tx_queues(dev, vi->curr\_queue\_pairs);

netif\_set\_real\_num\_rx_queues(dev, vi->curr\_queue\_pairs);

virtnet\_init\_settings(dev);

err = register_netdev(dev);

virtio\_device\_ready(vdev);

virtnet\_set\_queues(vi, vi->curr\_queue\_pairs);

......

}

在 virtnet\_probe 中，会创建 struct net\_device，并且通过 register_netdev 注册这个网络设备，这样在客户机里面，就能看到这个网卡了。

在 virtnet\_probe 中，还有一件重要的事情就是，init\_vqs 会初始化发送和接收的 virtqueue。

static int init_vqs(struct 

 virtnet_info *vi)

{

int ret;

ret = virtnet\_alloc\_queues(vi);

ret = virtnet\_find\_vqs(vi);

......

get\_online\_cpus();

virtnet\_set\_affinity(vi);

put\_online\_cpus();

return 

 0;

}

static int virtnet\_alloc\_queues(struct 

 virtnet_info *vi)

{

int i;

vi->sq = kzalloc(sizeof(*vi->sq) * vi->max\_queue\_pairs, GFP_KERNEL);

vi->rq = kzalloc(sizeof(*vi->rq) * vi->max\_queue\_pairs, GFP_KERNEL);

INIT\_DELAYED\_WORK(&vi->refill, refill_work);

for (i = 0; i < vi->max\_queue\_pairs; i++) {

vi->rq\[i\].pages = NULL;

netif\_napi\_add(vi->dev, &vi->rq\[i\].napi, virtnet_poll,

napi_weight);

netif\_tx\_napi_add(vi->dev, &vi->sq\[i\].napi, virtnet\_poll\_tx,

napi\_tx ? napi\_weight : 0);

sg\_init\_table(vi->rq\[i\].sg, ARRAY_SIZE(vi->rq\[i\].sg));

ewma\_pkt\_len_init(&vi->rq\[i\].mrg\_avg\_pkt_len);

sg\_init\_table(vi->sq\[i\].sg, ARRAY_SIZE(vi->sq\[i\].sg));

}

return 

 0;

}

按照上一节的 virtio 原理，virtqueue 是一个介于客户机前端和 qemu 后端的一个结构，用于在这两端之间传递数据，对于网络设备来讲有发送和接收两个方向的队列。这里建立的 struct virtqueue 是客户机前端对于队列的管理的数据结构。

队列的实体需要通过函数 virtnet\_find\_vqs 查找或者生成，这里还会指定接收队列的 callback 函数为 skb\_recv\_done，发送队列的 callback 函数为 skb\_xmit\_done。那当 buffer 使用发生变化的时候，我们可以调用这个 callback 函数进行通知。

static int virtnet\_find\_vqs(struct 

 virtnet_info *vi)

{

vq\_callback\_t **callbacks;

struct 

 virtqueue **vqs;

int ret = -ENOMEM;

int i, total_vqs;

const 

 char **names;

vqs = kzalloc(total_vqs * sizeof(*vqs), GFP_KERNEL);

callbacks = kmalloc(total_vqs * sizeof(*callbacks), GFP_KERNEL);

names = kmalloc(total_vqs * sizeof(*names), GFP_KERNEL);

for (i = 0; i < vi->max\_queue\_pairs; i++) {

callbacks\[rxq2vq(i)\] = skb\_recv\_done;

callbacks\[txq2vq(i)\] = skb\_xmit\_done;

names\[rxq2vq(i)\] = vi->rq\[i\].name;

names\[txq2vq(i)\] = vi->sq\[i\].name;

}

ret = vi->vdev->config->find_vqs(vi->vdev, total_vqs, vqs, callbacks, names, ctx, NULL);

......

for (i = 0; i < vi->max\_queue\_pairs; i++) {

vi->rq\[i\].vq = vqs\[rxq2vq(i)\];

vi->rq\[i\].min\_buf\_len = mergeable\_min\_buf_len(vi, vi->rq\[i\].vq);

vi->sq\[i\].vq = vqs\[txq2vq(i)\];

}

......

}

这里的 find\_vqs 是在 struct virtnet\_info 里的 struct virtio\_device 里的 struct virtio\_config_ops *config 里面定义的。

根据 virtio\_config\_ops 的定义，find\_vqs 会调用 vp\_modern\_find\_vqs，到这一步和块设备是一样的了。

在 vp\_modern\_find\_vqs 中，vp\_find\_vqs 会调用 vp\_find\_vqs\_intx。在 vp\_find\_vqs\_intx 中，通过 request\_irq 注册一个中断处理函数 vp\_interrupt。当设备向队列中写入信息时，会产生一个中断，也就是 vq 中断。中断处理函数需要调用相应的队列的回调函数，然后根据队列的数目，依次调用 vp\_setup_vq 完成 virtqueue、vring 的分配和初始化。

同样，这些数据结构会和 virtio 后端的 VirtIODevice、VirtQueue、vring 对应起来，都应该指向刚才创建的那一段内存。

客户机同样会通过调用专门给外部设备发送指令的函数 iowrite 告诉外部的 pci 设备，这些共享内存的地址。

至此前端设备驱动和后端设备驱动之间的两个收发队列就关联好了，这两个队列的格式和块设备是一样的。

## 发送网络包过程

接下来，我们来看当真的发送一个网络包的时候，会发生什么。

当网络包经过客户机的协议栈到达 virtio\_net 驱动的时候，按照 net\_device\_ops 的定义，start\_xmit 会被调用。

static 

 const 

 struct 

 net\_device\_ops virtnet_netdev = {

.ndo\_open = virtnet\_open,

.ndo\_stop = virtnet\_close,

.ndo\_start\_xmit = start_xmit,

.ndo\_validate\_addr = eth\_validate\_addr,

.ndo\_set\_mac\_address = virtnet\_set\_mac\_address,

.ndo\_set\_rx\_mode = virtnet\_set\_rx\_mode,

.ndo\_get\_stats64 = virtnet_stats,

.ndo\_vlan\_rx\_add\_vid = virtnet\_vlan\_rx\_add\_vid,

.ndo\_vlan\_rx\_kill\_vid = virtnet\_vlan\_rx\_kill\_vid,

.ndo\_xdp = virtnet\_xdp,

.ndo\_features\_check = passthru\_features\_check,

};

接下来的调用链为：start\_xmit->xmit\_skb-> virtqueue\_add\_outbuf->virtqueue\_add，将网络包放入队列中，并调用 virtqueue\_notify 通知接收方。

static 

 netdev\_tx\_t 

 start_xmit(struct sk_buff *skb, struct net_device *dev)

{

struct 

 virtnet_info *vi = netdev_priv(dev);

int qnum = skb\_get\_queue_mapping(skb);

struct 

 send_queue *sq = &vi->sq\[qnum\];

int err;

struct 

 netdev_queue *txq = netdev\_get\_tx_queue(dev, qnum);

bool kick = !skb->xmit_more;

bool use_napi = sq->napi.weight;

......

err = xmit_skb(sq, skb);

......

if (kick || netif\_xmit\_stopped(txq))

virtqueue_kick(sq->vq);

return NETDEV\_TX\_OK;

}

bool 

 virtqueue_kick(struct virtqueue *vq)

{

if (virtqueue\_kick\_prepare(vq))

return 

 virtqueue_notify(vq);

return 

 true;

}

写入一个 I/O 会使得 qemu 触发 VM exit，这个逻辑我们在解析 CPU 的时候看到过。

接下来，我们那会调用 VirtQueue 的 handle\_output 函数。前面我们已经设置过这个函数了，其实就是 virtio\_net\_handle\_tx_bh。

static 

 void 

 virtio\_net\_handle\_tx\_bh(VirtIODevice \*vdev, VirtQueue \*vq)

{

VirtIONet *n = VIRTIO_NET(vdev);

VirtIONetQueue *q = &n->vqs\[vq2q(virtio\_get\_queue_index(vq))\];

q->tx_waiting = 1;

virtio\_queue\_set_notification(vq, 0);

qemu\_bh\_schedule(q->tx_bh);

}

virtio\_net\_handle\_tx\_bh 调用了 qemu\_bh\_schedule，而在 virtio\_net\_add\_queue 中调用 qemu\_bh\_new，并把函数设置为 virtio\_net\_tx\_bh。

virtio\_net\_tx\_bh 函数调用发送函数 virtio\_net\_flush\_tx。

static 

 int32_t 

 virtio\_net\_flush_tx(VirtIONetQueue *q)

{

VirtIONet *n = q->n;

VirtIODevice *vdev = VIRTIO_DEVICE(n);

VirtQueueElement *elem;

int32_t num_packets = 0;

int queue_index = vq2q(virtio\_get\_queue_index(q->tx_vq));

for (;;) {

ssize_t ret;

unsigned 

 int out_num;

struct 

 iovec sg\[VIRTQUEUE\_MAX\_SIZE\], sg2\[VIRTQUEUE\_MAX\_SIZE + 1\], *out_sg;

struct 

 virtio\_net\_hdr\_mrg\_rxbuf mhdr;

elem = virtqueue_pop(q->tx_vq, sizeof(VirtQueueElement));

out\_num = elem->out\_num;

out\_sg = elem->out\_sg;

......

ret = qemu\_sendv\_packet_async(qemu\_get\_subqueue(n->nic, queue\_index),out\_sg, out\_num, virtio\_net\_tx\_complete);

}

......

return num_packets;

}

virtio\_net\_flush\_tx 会调用 virtqueue\_pop。这里面，我们能看到对于 vring 的操作，也即从这里面将客户机里面写入的数据读取出来。

然后，我们调用 qemu\_sendv\_packet\_async 发送网络包。接下来的调用链为：qemu\_sendv\_packet\_async->qemu\_net\_queue\_send\_iov->qemu\_net\_queue\_flush->qemu\_net\_queue\_deliver。

在 qemu\_net\_queue\_deliver 中，我们会调用 NetQueue 的 deliver 函数。前面 qemu\_new\_net\_queue 会把 deliver 函数设置为 qemu\_deliver\_packet\_iov。它会调用 nc->info->receive\_iov。

static 

 NetClientInfo 

 net\_tap\_info 

 = {

.type = NET\_CLIENT\_DRIVER_TAP,

.size = sizeof(TAPState),

.receive = tap_receive,

.receive\_raw = tap\_receive_raw,

.receive\_iov = tap\_receive_iov,

.poll = tap_poll,

.cleanup = tap_cleanup,

.has\_ufo = tap\_has_ufo,

.has\_vnet\_hdr = tap\_has\_vnet_hdr,

.has\_vnet\_hdr\_len = tap\_has\_vnet\_hdr_len,

.using\_vnet\_hdr = tap\_using\_vnet_hdr,

.set\_offload = tap\_set_offload,

.set\_vnet\_hdr\_len = tap\_set\_vnet\_hdr_len,

.set\_vnet\_le = tap\_set\_vnet_le,

.set\_vnet\_be = tap\_set\_vnet_be,

};

根据 net\_tap\_info 的定义调用的是 tap\_receive\_iov。他会调用 tap\_write\_packet->writev 写入这个字符设备。

在内核的字符设备驱动中，tun\_chr\_write_iter 会被调用。

static 

 ssize_t 

 tun\_chr\_write_iter(struct kiocb *iocb, struct iov_iter *from)

{

struct 

 file *file = iocb->ki_filp;

struct 

 tun_struct *tun = tun_get(file);

struct 

 tun_file *tfile = file->private_data;

ssize_t result;

result = tun\_get\_user(tun, tfile, NULL, from,

file->f\_flags & O\_NONBLOCK, false);

tun_put(tun);

return result;

}

当我们使用 writev() 系统调用向 tun/tap 设备的字符设备文件写入数据时，tun\_chr\_write 函数将被调用。它会使用 tun\_get\_user，从用户区接收数据，将数据存入 skb 中，然后调用关键的函数 netif\_rx\_ni(skb) ，将 skb 送给 tcp/ip 协议栈处理，最终完成虚拟网卡的数据接收。

至此，从虚拟机内部到宿主机的网络传输过程才算结束。

## 总结时刻

最后，我们把网络虚拟化场景下网络包的发送过程总结一下。

在虚拟机里面的用户态，应用程序通过 write 系统调用写入 socket。

写入的内容经过 VFS 层，内核协议栈，到达虚拟机里面的内核的网络设备驱动，也即 virtio_net。

virtio\_net 网络设备有一个操作结构 struct net\_device\_ops，里面定义了发送一个网络包调用的函数为 start\_xmit。

在 virtio\_net 的前端驱动和 qemu 中的后端驱动之间，有两个队列 virtqueue，一个用于发送，一个用于接收。然后，我们需要在 start\_xmit 中调用 virtqueue\_add，将网络包放入发送队列，然后调用 virtqueue\_notify 通知 qemu。

qemu 本来处于 KVM\_RUN 的状态，收到通知后，通过 VM exit 指令退出客户机模式，进入宿主机模式。发送网络包的时候，virtio\_net\_handle\_tx_bh 函数会被调用。

接下来是一个 for 循环，我们需要在循环中调用 virtqueue\_pop，从传输队列中获取要发送的数据，然后调用 qemu\_sendv\_packet\_async 进行发送。

qemu 会调用 writev 向字符设备文件写入，进入宿主机的内核。

在宿主机内核中字符设备文件的 file\_operations 里面的 write\_iter 会被调用，也即会调用 tun\_chr\_write_iter。

在 tun\_chr\_write\_iter 函数中，tun\_get\_user 将要发送的网络包从 qemu 拷贝到宿主机内核里面来，然后调用 netif\_rx_ni 开始调用宿主机内核协议栈进行处理。

宿主机内核协议栈处理完毕之后，会发送给 tap 虚拟网卡，完成从虚拟机里面到宿主机的整个发送过程。

![[e329505cfcd367612f8ae47054ec8e44_cee6cc66794947bea.jpg]]

## 课堂练习

这一节我们解析的是发送过程，请你根据类似的思路，解析一下接收过程。

欢迎留言和我分享你的疑惑和见解，也欢迎收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。

![[8c0a95fa07a8b9a1abfd394479bdd637_54536e4566e949e98.jpg]]