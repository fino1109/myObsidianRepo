上一节我们讲网络包的发送，讲了上半部分，也即从 VFS 层一直到 IP 层，这一节我们接着看下去，看 IP 层和 MAC 层是如何发送数据的。

## 解析 ip\_queue\_xmit 函数

从 ip\_queue\_xmit 函数开始，我们就要进入 IP 层的发送逻辑了。

int ip\_queue\_xmit(struct 

 sock *sk, struct 

 sk_buff *skb, struct 

 flowi *fl)

{

struct 

 inet_sock *inet = inet_sk(sk);

struct 

 net *net = sock_net(sk);

struct 

 ip\_options\_rcu *inet_opt;

struct 

 flowi4 *fl4;

struct 

 rtable *rt;

struct 

 iphdr *iph;

int res;

inet_opt = rcu_dereference(inet->inet_opt);

fl4 = &fl->u.ip4;

rt = skb_rtable(skb);

rt = (struct 

 rtable *)\_\_sk\_dst_check(sk, 0);

if (!rt) {

__be32 daddr;

daddr = inet->inet_daddr;

......

rt = ip\_route\_output_ports(net, fl4, sk,

daddr, inet->inet_saddr,

inet->inet_dport,

inet->inet_sport,

sk->sk_protocol,

RT\_CONN\_FLAGS(sk),

sk->sk\_bound\_dev_if);

if (IS_ERR(rt))

goto no_route;

sk\_setup\_caps(sk, &rt->dst);

}

skb\_dst\_set_noref(skb, &rt->dst);

packet_routed:

skb_push(skb, sizeof(struct 

 iphdr) \+ (inet\_opt ? inet\_opt->opt.optlen : 0));

skb\_reset\_network_header(skb);

iph = ip_hdr(skb);

*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));

if (ip\_dont\_fragment(sk, &rt->dst) && !skb->ignore_df)

iph->frag_off = htons(IP_DF);

else

iph->frag_off = 0;

iph->ttl = ip\_select\_ttl(inet, &rt->dst);

iph->protocol = sk->sk_protocol;

ip\_copy\_addrs(iph, fl4);

if (inet\_opt && inet\_opt->opt.optlen) {

iph->ihl += inet_opt->opt.optlen >> 2;

ip\_options\_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);

}

ip\_select\_ident_segs(net, skb, sk,

skb_shinfo(skb)->gso_segs ?: 1);

skb->priority = sk->sk_priority;

skb->mark = sk->sk_mark;

res = ip\_local\_out(net, sk, skb);

......

}

在 ip\_queue\_xmit 中，也即 IP 层的发送函数里面，有三部分逻辑。

第一部分，选取路由，也即我要发送这个包应该从哪个网卡出去。

这件事情主要由 ip\_route\_output\_ports 函数完成。接下来的调用链为：ip\_route\_output\_ports->ip\_route\_output\_flow->\_\_ip\_route\_output\_key->ip\_route\_output\_key\_hash->ip\_route\_output\_key\_hash\_rcu。

struct 

 rtable *ip\_route\_output\_key\_hash_rcu(struct net *net, struct flowi4 *fl4, struct fib_result *res, const 

 struct sk_buff *skb)

{

struct 

 net_device *dev_out = NULL;

int orig\_oif = fl4->flowi4\_oif;

unsigned 

 int flags = 0;

struct 

 rtable *rth;

......

err = fib_lookup(net, fl4, res, 0);

......

make_route:

rth = \_\_mkroute\_output(res, fl4, orig\_oif, dev\_out, flags);

......

}

ip\_route\_output\_key\_hash\_rcu 先会调用 fib\_lookup。

FIB 全称是 Forwarding Information Base，转发信息表。其实就是咱们常说的路由表。

static 

 inline 

 int 

 fib_lookup(struct net *net, const 

 struct flowi4 *flp, struct fib_result *res, unsigned 

 int flags)

{ struct 

 fib_table *tb;

......

tb = fib\_get\_table(net, RT\_TABLE\_MAIN);

if (tb)

err = fib\_table\_lookup(tb, flp, res, flags | FIB\_LOOKUP\_NOREF);

......

}

路由表可以有多个，一般会有一个主表，RT\_TABLE\_MAIN。然后 fib\_table\_lookup 函数在这个表里面进行查找。

路由表是一个什么样的结构呢？

路由就是在 Linux 服务器上的路由表里面配置的一条一条规则。这些规则大概是这样的：想访问某个网段，从某个网卡出去，下一跳是某个 IP。

之前我们讲过一个简单的拓扑图，里面的三台 Linux 机器的路由表都可以通过 ip route 命令查看。

![[f6982eb85dc66bd04200474efb3a050e_25e93d32a2a647a38.png]]

default via 192.168.1.1 dev eth0

192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.100 metric 100

default via 192.168.2.1 dev eth0

192.168.2.0/24 dev eth0 proto kernel scope link src 192.168.2.100 metric 100

192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.1

192.168.2.0/24 dev eth1 proto kernel scope link src 192.168.2.1

其实，对于两端的服务器来讲，我们没有太多路由可以选，但是对于中间的 Linux 服务器做路由器来讲，这里有两条路可以选，一个是往左面转发，一个是往右面转发，就需要路由表的查找。

fib\_table\_lookup 的代码逻辑比较复杂，好在注释比较清楚。因为路由表要按照前缀进行查询，希望找到最长匹配的那一个，例如 192.168.2.0/24 和 192.168.0.0/16 都能匹配 192.168.2.100/24。但是，我们应该使用 192.168.2.0/24 的这一条。

为了更方面的做这个事情，我们使用了 Trie 树这种结构。比如我们有一系列的字符串：{bcs#, badge#, baby#, back#, badger#, badness#}。之所以每个字符串都加上 #，是希望不要一个字符串成为另外一个字符串的前缀。然后我们把它们放在 Trie 树中，如下图所示：

![[3f0a99cf1c47afcd0bd740c4b7802511_f97c18866c944a1e9.png]]

对于将 IP 地址转成二进制放入 trie 树，也是同样的道理，可以很快进行路由的查询。

找到了路由，就知道了应该从哪个网卡发出去。

然后，ip\_route\_output\_key\_hash\_rcu 会调用 \_\_mkroute\_output，创建一个 struct rtable，表示找到的路由表项。这个结构是由 rt\_dst_alloc 函数分配的。

struct 

 rtable *rt\_dst\_alloc(struct 

 net_device *dev,

unsigned int flags, u16 

 type,

bool nopolicy, bool noxfrm, bool will_cache)

{

struct 

 rtable *rt;

rt = dst_alloc(&ipv4\_dst\_ops, dev, 1, DST\_OBSOLETE\_FORCE_CHK,

(will_cache ? 0 : DST_HOST) |

(nopolicy ? DST_NOPOLICY : 0) |

(noxfrm ? DST_NOXFRM : 0));

if (rt) {

rt->rt_genid = rt\_genid\_ipv4(dev_net(dev));

rt->rt_flags = flags;

rt->rt_type = type;

rt->rt\_is\_input = 0;

rt->rt_iif = 0;

rt->rt_pmtu = 0;

rt->rt_gateway = 0;

rt->rt\_uses\_gateway = 0;

rt->rt\_table\_id = 0;

INIT\_LIST\_HEAD(&rt->rt_uncached);

rt->dst.output = ip_output;

if (flags & RTCF_LOCAL)

rt->dst.input = ip\_local\_deliver;

}

return rt;

}

最终返回 struct rtable 实例，第一部分也就完成了。

第二部分，就是准备 IP 层的头，往里面填充内容。这就要对着 IP 层的头的格式进行理解。

![[6b2ea7148a8e04138a2228c5dbc7182b_970c563172204a9f9.png]]

在这里面，服务类型设置为 tos，标识位里面设置是否允许分片 frag\_off。如果不允许，而遇到 MTU 太小过不去的情况，就发送 ICMP 报错。TTL 是这个包的存活时间，为了防止一个 IP 包迷路以后一直存活下去，每经过一个路由器 TTL 都减一，减为零则“死去”。设置 protocol，指的是更上层的协议，这里是 TCP。源地址和目标地址由 ip\_copy_addrs 设置。最后，设置 options。

第三部分，就是调用 ip\_local\_out 发送 IP 包。

int 

 ip\_local\_out(struct net *net, struct sock *sk, struct sk_buff *skb)

{

int err;

err = \_\_ip\_local_out(net, sk, skb);

if (likely(err == 1))

err = dst_output(net, sk, skb);

return err;

}

int \_\_ip\_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)

{

struct 

 iphdr *iph = ip_hdr(skb);

iph->tot_len = htons(skb->len);

skb->protocol = htons(ETH\_P\_IP);

return 

 nf_hook(NFPROTO\_IPV4, NF\_INET\_LOCAL\_OUT,

net, sk, skb, NULL, skb_dst(skb)->dev,

dst_output);

}

ip\_local\_out 先是调用 \_\_ip\_local\_out，然后里面调用了 nf\_hook。这是什么呢？nf 的意思是 Netfilter，这是 Linux 内核的一个机制，用于在网络发送和转发的关键节点上加上 hook 函数，这些函数可以截获数据包，对数据包进行干预。

一个著名的实现，就是内核模块 ip_tables。在用户态，还有一个客户端程序 iptables，用命令行来干预内核的规则。

![[75c8257049eed99499e802fcc2eacf4d_806ec8f8489b425d9.png]]

iptables 有表和链的概念，最终要的是两个表。

filter 表处理过滤功能，主要包含以下三个链。

INPUT 链：过滤所有目标地址是本机的数据包

FORWARD 链：过滤所有路过本机的数据包

OUTPUT 链：过滤所有由本机产生的数据包

nat 表主要处理网络地址转换，可以进行 SNAT（改变源地址）、DNAT（改变目标地址），包含以下三个链。

PREROUTING 链：可以在数据包到达时改变目标地址

OUTPUT 链：可以改变本地产生的数据包的目标地址

POSTROUTING 链：在数据包离开时改变数据包的源地址

![[765e5431fe4b17f62b1b5712cc82abda_735ca109a6db4a318.png]]

在这里，网络包马上就要发出去了，因而是 NF\_INET\_LOCAL\_OUT，也即 ouput 链，如果用户曾经在 iptables 里面写过某些规则，就会在 nf\_hook 这个函数里面起作用。

ip\_local\_out 再调用 dst_output，就是真正的发送数据。

static 

 inline 

 int 

 dst_output(struct net *net, struct sock *sk, struct sk_buff *skb)

{

return 

 skb_dst(skb)->output(net, sk, skb);

}

这里调用的就是 struct rtable 成员 dst 的 ouput 函数。在 rt\_dst\_alloc 中，我们可以看到，output 函数指向的是 ip_output。

int ip_output(struct 

 net *net, struct 

 sock *sk, struct 

 sk_buff *skb)

{

struct 

 net_device *dev = skb_dst(skb)->dev;

skb->dev = dev;

skb->protocol = htons(ETH\_P\_IP);

return 

 NF\_HOOK\_COND(NFPROTO\_IPV4, NF\_INET\_POST\_ROUTING,

net, sk, skb, NULL, dev,

ip\_finish\_output,

!(IPCB(skb)->flags & IPSKB_REROUTED));

}

在 ip\_output 里面，我们又看到了熟悉的 NF\_HOOK。这一次是 NF\_INET\_POST\_ROUTING，也即 POSTROUTING 链，处理完之后，调用 ip\_finish_output。

## 解析 ip\_finish\_output 函数

从 ip\_finish\_output 函数开始，发送网络包的逻辑由第三层到达第二层。ip\_finish\_output 最终调用 ip\_finish\_output2。

static 

 int 

 ip\_finish\_output2(struct net *net, struct sock *sk, struct sk_buff *skb)

{

struct 

 dst_entry *dst = skb_dst(skb);

struct 

 rtable *rt = (struct rtable *)dst;

struct 

 net_device *dev = dst->dev;

unsigned 

 int hh_len = LL\_RESERVED\_SPACE(dev);

struct 

 neighbour *neigh;

u32 nexthop;

......

nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)->daddr);

neigh = \_\_ipv4\_neigh\_lookup\_noref(dev, nexthop);

if (unlikely(!neigh))

neigh = \_\_neigh\_create(&arp_tbl, &nexthop, dev, false);

if (!IS_ERR(neigh)) {

int res;

sock\_confirm\_neigh(skb, neigh);

res = neigh_output(neigh, skb);

return res;

}

......

}

在 ip\_finish\_output2 中，先找到 struct rtable 路由表里面的下一跳，下一跳一定和本机在同一个局域网中，可以通过二层进行通信，因而通过 \_\_ipv4\_neigh\_lookup\_noref，查找如何通过二层访问下一跳。

static 

 inline 

 struct 

 neighbour *\_\_ipv4\_neigh\_lookup\_noref(struct net_device *dev, u32 key)

{

return \_\_\_neigh\_lookup\_noref(&arp\_tbl, neigh\_key\_eq32, arp_hashfn, &key, dev);

}

\_\_ipv4\_neigh\_lookup\_noref 是从本地的 ARP 表中查找下一跳的 MAC 地址。ARP 表的定义如下：

struct neigh\_table arp\_tbl = {

.family = AF_INET,

.key_len = 4,

.protocol = cpu\_to\_be16(ETH\_P\_IP),

.hash = arp_hash,

.key\_eq = arp\_key_eq,

.constructor = arp_constructor,

.proxy\_redo = parp\_redo,

.id = "arp_cache",

......

.gc_interval = 30 \* HZ,

.gc_thresh1 = 128,

.gc_thresh2 = 512,

.gc_thresh3 = 1024,

};

如果在 ARP 表中没有找到相应的项，则调用 \_\_neigh\_create 进行创建。

struct 

 neighbour *\_\_neigh\_create(struct 

 neigh_table *tbl, const void *pkey, struct 

 net_device *dev, bool want_ref)

{

u32 hash_val;

int key_len = tbl->key_len;

int error;

struct 

 neighbour \*n1, \*rc, *n = neigh_alloc(tbl, dev);

struct 

 neigh\_hash\_table *nht;

memcpy(n->primary\_key, pkey, key\_len);

n->dev = dev;

dev_hold(dev);

if (tbl->constructor && (error = tbl->constructor(n)) < 0) {

......

}

......

if (atomic_read(&tbl->entries) > (1 << nht->hash_shift))

nht = neigh\_hash\_grow(tbl, nht->hash_shift + 1);

hash_val = tbl->hash(pkey, dev, nht->hash_rnd) >> (32 \- nht->hash_shift);

for (n1 = rcu\_dereference\_protected(nht->hash\_buckets\[hash\_val\],

lockdep\_is\_held(&tbl->lock));

n1 != NULL;

n1 = rcu\_dereference\_protected(n1->next,

lockdep\_is\_held(&tbl->lock))) {

if (dev == n1->dev && !memcmp(n1->primary\_key, pkey, key\_len)) {

if (want_ref)

neigh_hold(n1);

rc = n1;

goto out\_tbl\_unlock;

}

}

......

rcu\_assign\_pointer(n->next,

rcu\_dereference\_protected(nht->hash\_buckets\[hash\_val\],

lockdep\_is\_held(&tbl->lock)));

rcu\_assign\_pointer(nht->hash\_buckets\[hash\_val\], n);

......

}

\_\_neigh\_create 先调用 neigh_alloc，创建一个 struct neighbour 结构，用于维护 MAC 地址和 ARP 相关的信息。这个名字也很好理解，大家都是在一个局域网里面，可以通过 MAC 地址访问到，当然是邻居了。

static 

 struct 

 neighbour *neigh_alloc(struct 

 neigh_table *tbl, struct 

 net_device *dev)

{

struct 

 neighbour *n = NULL;

unsigned long now = jiffies;

int entries;

......

n = kzalloc(tbl->entry_size + dev->neigh\_priv\_len, GFP_ATOMIC);

if (!n)

goto out_entries;

\_\_skb\_queue\_head\_init(&n->arp_queue);

rwlock_init(&n->lock);

seqlock_init(&n->ha_lock);

n->updated = n->used = now;

n->nud\_state = NUD\_NONE;

n->output = neigh_blackhole;

seqlock_init(&n->hh.hh_lock);

n->parms = neigh\_parms\_clone(&tbl->parms);

setup_timer(&n->timer, neigh\_timer\_handler, (unsigned long)n);

NEIGH\_CACHE\_STAT_INC(tbl, allocs);

n->tbl = tbl;

refcount_set(&n->refcnt, 1);

n->dead = 1;

......

}

在 neigh\_alloc 中，我们先分配一个 struct neighbour 结构并且初始化。这里面比较重要的有两个成员，一个是 arp\_queue，所以上层想通过 ARP 获取 MAC 地址的任务，都放在这个队列里面。另一个是 timer 定时器，我们设置成，过一段时间就调用 neigh\_timer\_handler，来处理这些 ARP 任务。

\_\_neigh\_create 然后调用了 arp\_tbl 的 constructor 函数，也即调用了 arp\_constructor，在这里面定义了 ARP 的操作 arp\_hh\_ops。

static int arp_constructor(struct 

 neighbour *neigh)

{

\_\_be32 addr = *(\_\_be32 *)neigh->primary_key;

struct 

 net_device *dev = neigh->dev;

struct 

 in_device *in_dev;

struct 

 neigh_parms *parms;

......

neigh->type = inet\_addr\_type\_dev\_table(dev_net(dev), dev, addr);

parms = in_dev->arp_parms;

\_\_neigh\_parms_put(neigh->parms);

neigh->parms = neigh\_parms\_clone(parms);

......

neigh->ops = &arp\_hh\_ops;

......

neigh->output = neigh->ops->output;

......

}

static 

 const 

 struct 

 neigh_ops arp\_hh\_ops = {

.family = AF_INET,

.solicit = arp_solicit,

.error\_report = arp\_error_report,

.output = neigh\_resolve\_output,

.connected\_output = neigh\_resolve_output,

};

\_\_neigh\_create 最后是将创建的 struct neighbour 结构放入一个哈希表，从里面的代码逻辑比较容易看出，这是一个数组加链表的链式哈希表，先计算出哈希值 hash_val，得到相应的链表，然后循环这个链表找到对应的项，如果找不到就在最后插入一项。

我们回到 ip\_finish\_output2，在 \_\_neigh\_create 之后，会调用 neigh_output 发送网络包。

static 

 inline 

 int 

 neigh_output(struct neighbour *n, struct sk_buff *skb)

{

......

return n->output(n, skb);

}

按照上面对于 struct neighbour 的操作函数 arp\_hh\_ops 的定义，output 调用的是 neigh\_resolve\_output。

int 

 neigh\_resolve\_output(struct neighbour *neigh, struct sk_buff *skb)

{

if (!neigh\_event\_send(neigh, skb)) {

......

rc = dev\_queue\_xmit(skb);

}

......

}

在 neigh\_resolve\_output 里面，首先 neigh\_event\_send 触发一个事件，看能否激活 ARP。

int \_\_neigh\_event_send(struct 

 neighbour *neigh, struct 

 sk_buff *skb)

{

int rc;

bool immediate_probe = false;

if (!(neigh->nud\_state & (NUD\_STALE | NUD_INCOMPLETE))) {

if (NEIGH_VAR(neigh->parms, MCAST_PROBES) +

NEIGH_VAR(neigh->parms, APP_PROBES)) {

unsigned long next, now = jiffies;

atomic_set(&neigh->probes,

NEIGH_VAR(neigh->parms, UCAST_PROBES));

neigh->nud\_state = NUD\_INCOMPLETE;

neigh->updated = now;

next = now + max(NEIGH_VAR(neigh->parms, RETRANS_TIME),

HZ/2);

neigh\_add\_timer(neigh, next);

immediate_probe = true;

}

......

} else 

 if (neigh->nud\_state & NUD\_STALE) {

neigh_dbg(2, "neigh %p is delayed\\n", neigh);

neigh->nud\_state = NUD\_DELAY;

neigh->updated = jiffies;

neigh\_add\_timer(neigh, jiffies +

NEIGH_VAR(neigh->parms, DELAY\_PROBE\_TIME));

}

if (neigh->nud\_state == NUD\_INCOMPLETE) {

if (skb) {

.......

\_\_skb\_queue_tail(&neigh->arp_queue, skb);

neigh->arp\_queue\_len_Bytes += skb->truesize;

}

rc = 1;

}

out\_unlock\_bh:

if (immediate_probe)

neigh_probe(neigh);

.......

}

在 \_\_neigh\_event\_send 中，激活 ARP 分两种情况，第一种情况是马上激活，也即 immediate\_probe。另一种情况是延迟激活则仅仅设置一个 timer。然后将 ARP 包放在 arp\_queue 上。如果马上激活，就直接调用 neigh\_probe；如果延迟激活，则定时器到了就会触发 neigh\_timer\_handler，在这里面还是会调用 neigh_probe。

我们就来看 neigh\_probe 的实现，在这里面会从 arp\_queue 中拿出 ARP 包来，然后调用 struct neighbour 的 solicit 操作。

static void neigh_probe(struct 

 neighbour *neigh)

__releases(neigh->lock)

{

struct 

 sk_buff *skb = skb\_peek\_tail(&neigh->arp_queue);

......

if (neigh->ops->solicit)

neigh->ops->solicit(neigh, skb);

......

}

按照上面对于 struct neighbour 的操作函数 arp\_hh\_ops 的定义，solicit 调用的是 arp\_solicit，在这里我们可以找到对于 arp\_send\_dst 的调用，创建并发送一个 arp 包，得到结果放在 struct dst\_entry 里面。

static 

 void 

 arp\_send\_dst(int type, int ptype, \_\_be32 dest\_ip,

struct net\_device *dev, \_\_be32 src_ip,

const 

 unsigned 

 char *dest_hw,

const 

 unsigned 

 char *src_hw,

const 

 unsigned 

 char *target_hw,

struct dst_entry *dst)

{

struct 

 sk_buff *skb;

......

skb = arp_create(type, ptype, dest\_ip, dev, src\_ip,

dest\_hw, src\_hw, target_hw);

......

skb\_dst\_set(skb, dst_clone(dst));

arp_xmit(skb);

}

我们回到 neigh\_resolve\_output 中，当 ARP 发送完毕，就可以调用 dev\_queue\_xmit ��送二层网络包了。

\* \_\_dev\_queue_xmit - transmit a buffer

\* @skb: buffer to transmit

\* @accel_priv: private data used for L2 forwarding offload

*

\* Queue a buffer for transmission to a network device.

*/

static 

 int \_\_dev\_queue_xmit(struct sk_buff *skb, void *accel_priv)

{

struct net_device *dev = skb->dev;

struct netdev_queue *txq;

struct Qdisc *q;

......

txq = netdev\_pick\_tx(dev, skb, accel_priv);

q = rcu\_dereference\_bh(txq->qdisc);

if (q->enqueue) {

rc = \_\_dev\_xmit_skb(skb, q, dev, txq);

goto 

 out;

}

......

}

就像咱们在讲述硬盘块设备的时候讲过，每个块设备都有队列，用于将内核的数据放到队列里面，然后设备驱动从队列里面取出后，将数据根据具体设备的特性发送给设备。

网络设备也是类似的，对于发送来说，有一个发送队列 struct netdev_queue *txq。

这里还有另一个变量叫做 struct Qdisc，这个是什么呢？如果我们在一台 Linux 机器上运行 ip addr，我们能看到对于一个网卡，都有下面的输出。

\# ip addr

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN 

 group 

 default qlen 1000

link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

inet 127.0.0.1/8 

 scope host lo

valid\_lft forever preferred\_lft forever

inet6 ::1/128 

 scope host

valid\_lft forever preferred\_lft forever

2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc pfifo_fast state UP group 

 default qlen 1000

link/ether fa:16:3e:75:99:08 brd ff:ff:ff:ff:ff:ff

inet 10.173.32.47/21 brd 10.173.39.255 

 scope 

 global noprefixroute dynamic eth0

valid_lft 67104sec preferred_lft 67104sec

inet6 fe80::f816:3eff:fe75:9908/64 

 scope link

valid\_lft forever preferred\_lft forever

这里面有个关键字 qdisc pfifo_fast 是什么意思呢？qdisc 全称是 queueing discipline，中文叫排队规则。内核如果需要通过某个网络接口发送数据包，都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。

最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。

三个波段的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。

数据包是按照服务类型（Type of Service，TOS）被分配到三个波段里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。

pfifo_fast 分为三个先入先出的队列，我们能称为三个 Band。根据网络包里面的 TOS，看这个包到底应该进入哪个队列。TOS 总共四位，每一位表示的意思不同，总共十六种类型。

![[ab6af2f9e1a64868636080a05cfde0d9_d6777834065941deb.png]]

通过命令行 tc qdisc show dev eth0，我们可以输出结果 priomap，也是十六个数字。在 0 到 2 之间，和 TOS 的十六种类型对应起来。不同的 TOS 对应不同的队列。其中 Band 0 优先级最高，发送完毕后才轮到 Band 1 发送，最后才是 Band 2。

qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap 1 

 2 

 2 

 2 

 1 

 2 

 0 

 0 

 1 

 1 

 1 

 1 

 1 

 1 

 1 

 1

接下来，\_\_dev\_xmit_skb 开始进行网络包发送。

static 

 inline 

 int \_\_dev\_xmit_skb(struct sk_buff *skb, struct Qdisc *q,

struct net_device *dev,

struct netdev_queue *txq)

{

......

rc = q->enqueue(skb, q, &to\_free) & NET\_XMIT_MASK;

if (qdisc\_run\_begin(q)) {

......

\_\_qdisc\_run(q);

}

......

}

void \_\_qdisc\_run(struct Qdisc *q)

{

int quota = dev\_tx\_weight;

int packets;

while (qdisc_restart(q, &packets)) {

\* Ordered by possible occurrence: Postpone processing if

\* 1\. we've exceeded packet quota

\* 2\. another process needs the CPU;

*/

quota -= packets;

if (quota <= 0 || need_resched()) {

\_\_netif\_schedule(q);

break;

}

}

qdisc\_run\_end(q);

}

\_\_dev\_xmit\_skb 会将请求放入队列，然后调用 \_\_qdisc\_run 处理队列中的数据。qdisc\_restart 用于数据的发送。根据注释中的说法，qdisc 的另一个功能是用于控制网络包的发送速度，因而如果超过速度，就需要重新调度，则会调用 \_\_netif\_schedule。

static 

 void \_\_netif\_reschedule(struct Qdisc *q)

{

struct 

 softnet_data *sd;

unsigned 

 long flags;

local\_irq\_save(flags);

sd = this\_cpu\_ptr(&softnet_data);

q->next_sched = NULL;

*sd->output\_queue\_tailp = q;

sd->output\_queue\_tailp = &q->next_sched;

raise\_softirq\_irqoff(NET\_TX\_SOFTIRQ);

local\_irq\_restore(flags);

}

\_\_netif\_schedule 会调用 \_\_netif\_reschedule，发起一个软中断 NET\_TX\_SOFTIRQ。咱们讲设备驱动程序的时候讲过，设备驱动程序处理中断，分两个过程，一个是屏蔽中断的关键处理逻辑，一个是延迟处理逻辑。当时说工作队列是延迟处理逻辑的处理方案，软中断也是一种方案。

在系统初始化的时候，我们会定义软中断的处理函数。例如，NET\_TX\_SOFTIRQ 的处理函数是 net\_tx\_action，用于发送网络包。还有一个 NET\_RX\_SOFTIRQ 的处理函数是 net\_rx\_action，用于接收网络包。接收网络包的过程咱们下一节解析。

open\_softirq(NET\_TX\_SOFTIRQ, net\_tx_action);

open\_softirq(NET\_RX\_SOFTIRQ, net\_rx_action);

这里我们来解析一下 net\_tx\_action。

static \_\_latent\_entropy void net\_tx\_action(struct 

 softirq_action *h)

{

struct 

 softnet_data *sd = this\_cpu\_ptr(&softnet_data);

......

if (sd->output_queue) {

struct 

 Qdisc *head;

local\_irq\_disable();

head = sd->output_queue;

sd->output_queue = NULL;

sd->output\_queue\_tailp = &sd->output_queue;

local\_irq\_enable();

while (head) {

struct 

 Qdisc *q = head;

spinlock\_t *root\_lock;

head = head->next_sched;

......

qdisc_run(q);

}

}

}

我们会发现，net\_tx\_action 还是调用了 qdisc\_run，还是会调用 \_\_qdisc\_run，然后调用 qdisc\_restart 发送网络包。

我们来看一下 qdisc_restart 的实现。

static 

 inline 

 int 

 qdisc_restart(struct Qdisc *q, int *packets)

{

struct 

 netdev_queue *txq;

struct 

 net_device *dev;

spinlock_t *root_lock;

struct 

 sk_buff *skb;

bool validate;

skb = dequeue_skb(q, &validate, packets);

if (unlikely(!skb))

return 

 0;

root_lock = qdisc_lock(q);

dev = qdisc_dev(q);

txq = skb\_get\_tx_queue(dev, skb);

return 

 sch\_direct\_xmit(skb, q, dev, txq, root_lock, validate);

}

qdisc\_restart 将网络包从 Qdisc 的队列中拿下来，然后调用 sch\_direct_xmit 进行发送。

int 

 sch\_direct\_xmit(struct sk_buff *skb, struct Qdisc *q,

struct net_device *dev, struct netdev_queue *txq,

spinlock_t *root_lock, bool validate)

{

int ret = NETDEV\_TX\_BUSY;

if (likely(skb)) {

if (!netif\_xmit\_frozen\_or\_stopped(txq))

skb = dev\_hard\_start_xmit(skb, dev, txq, &ret);

}

......

if (dev\_xmit\_complete(ret)) {

ret = qdisc_qlen(q);

} else {

ret = dev\_requeue\_skb(skb, q);

}

......

}

在 sch\_direct\_xmit 中，调用 dev\_hard\_start\_xmit 进行发送，如果发送不成功，会返回 NETDEV\_TX\_BUSY。这说明网络卡很忙，于是就调用 dev\_requeue_skb，重新放入队列。

struct 

 sk_buff *dev\_hard\_start_xmit(struct sk_buff *first, struct net_device *dev, struct netdev_queue *txq, int *ret)

{

struct 

 sk_buff *skb = first;

int rc = NETDEV\_TX\_OK;

while (skb) {

struct 

 sk_buff *next = skb->next;

rc = xmit_one(skb, dev, txq, next != NULL);

skb = next;

if (netif\_xmit\_stopped(txq) && skb) {

rc = NETDEV\_TX\_BUSY;

break;

}

}

......

}

在 dev\_hard\_start\_xmit 中，是一个 while 循环。每次在队列中取出一个 sk\_buff，调用 xmit_one 发送。

接下来的调用链为：xmit\_one->netdev\_start\_xmit->\_\_netdev\_start\_xmit。

static 

 inline 

 netdev\_tx\_t \_\_netdev\_start_xmit(const 

 struct net\_device\_ops *ops, struct sk_buff *skb, struct net_device *dev, bool more)

{

skb->xmit_more = more ? 1 : 0;

return ops->ndo\_start\_xmit(skb, dev);

}

这个时候，已经到了设备驱动层了。我们能看到，drivers/net/ethernet/intel/ixgb/ixgb_main.c 里面有对于这个网卡的操作的定义。

static 

 const 

 struct 

 net\_device\_ops ixgb\_netdev\_ops = {

.ndo\_open = ixgb\_open,

.ndo\_stop = ixgb\_close,

.ndo\_start\_xmit = ixgb\_xmit\_frame,

.ndo\_set\_rx\_mode = ixgb\_set_multi,

.ndo\_validate\_addr = eth\_validate\_addr,

.ndo\_set\_mac\_address = ixgb\_set_mac,

.ndo\_change\_mtu = ixgb\_change\_mtu,

.ndo\_tx\_timeout = ixgb\_tx\_timeout,

.ndo\_vlan\_rx\_add\_vid = ixgb\_vlan\_rx\_add\_vid,

.ndo\_vlan\_rx\_kill\_vid = ixgb\_vlan\_rx\_kill\_vid,

.ndo\_fix\_features = ixgb\_fix\_features,

.ndo\_set\_features = ixgb\_set\_features,

};

在这里面，我们可以找到对于 ndo\_start\_xmit 的定义，调用 ixgb\_xmit\_frame。

static 

 netdev\_tx\_t

ixgb\_xmit\_frame(struct sk_buff *skb, struct net_device *netdev)

{

struct 

 ixgb_adapter *adapter = netdev_priv(netdev);

......

if (count) {

ixgb\_tx\_queue(adapter, count, vlan\_id, tx\_flags);

ixgb\_maybe\_stop_tx(netdev, &adapter->tx\_ring, DESC\_NEEDED);

}

......

return NETDEV\_TX\_OK;

}

在 ixgb\_xmit\_frame 中，我们会得到这个网卡对应的适配器，然后将其放入硬件网卡的队列中。

至此，整个发送才算结束。

## 总结时刻

这一节，我们继续解析了发送一个网络包的过程，我们整个过程的图画在了下面。

![[79cc42f3163d159a66e163c006d9f36f_dba2c01212bc48d7a.png]]

这个过程分成几个层次。

VFS 层：write 系统调用找到 struct file，根据里面的 file\_operations 的定义，调用 sock\_write\_iter 函数。sock\_write\_iter 函数调用 sock\_sendmsg 函数。

Socket 层：从 struct file 里面的 private\_data 得到 struct socket，根据里面 ops 的定义，调用 inet\_sendmsg 函数。

Sock 层：从 struct socket 里面的 sk 得到 struct sock，根据里面 sk\_prot 的定义，调用 tcp\_sendmsg 函数。

TCP 层：tcp\_sendmsg 函数会调用 tcp\_write\_xmit 函数，tcp\_write\_xmit 函数会调用 tcp\_transmit_skb，在这里实现了 TCP 层面向连接的逻辑。

IP 层：扩展 struct sock，得到 struct inet\_connection\_sock，根据里面 icsk\_af\_ops 的定义，调用 ip\_queue\_xmit 函数。

IP 层：ip\_route\_output\_ports 函数里面会调用 fib\_lookup 查找路由表。FIB 全称是 Forwarding Information Base，转发信息表，也就是路由表。

在 IP 层里面要做的另一个事情是填写 IP 层的头。

在 IP 层还要做的一件事情就是通过 iptables 规则。

MAC 层：IP 层调用 ip\_finish\_output 进行 MAC 层。

MAC 层需要 ARP 获得 MAC 地址，因而要调用 \_\_\_neigh\_lookup\_noref 查找属于同一个网段的邻居，他会调用 neigh\_probe 发送 ARP。

有了 MAC 地址，就可以调用 dev\_queue\_xmit 发送二层网络包了，它会调用 \_\_dev\_xmit_skb 会将请求放入队列。

设备层：网络包的发送会触发一个软中断 NET\_TX\_SOFTIRQ 来处理队列中的数据。这个软中断的处理函数是 net\_tx\_action。

在软中断处理函数中，会将网络包从队列上拿下来，调用网络设备的传输函数 ixgb\_xmit\_frame，将网络包发到设备的队列上去。

## 课堂练习

上一节你应该通过 tcpdump 看到了 TCP 包头的格式，这一节，请你查看一下 IP 包的格式以及 ARP 的过程。

欢迎留言和我分享你的疑惑和见解 ，也欢迎可以收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。

![[8c0a95fa07a8b9a1abfd394479bdd637_a957f15a42714c7eb.jpg]]