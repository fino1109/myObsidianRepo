在文件系统那一节，我们讲了文件的写入，到了设备驱动这一层，就没有再往下分析。上一节我们又讲了 mount 一个块设备，将 block\_device 信息放到了 ext4 文件系统的 super\_block 里面，有了这些基础，是时候把整个写入的故事串起来了。

还记得咱们在文件系统那一节分析写入流程的时候，对于 ext4 文件系统，最后调用的是 ext4\_file\_write_iter，它将 I/O 的调用分成两种情况：

第一是直接 I/O。最终我们调用的是 generic\_file\_direct\_write，这里调用的是 mapping->a\_ops->direct\_IO，实际调用的是 ext4\_direct_IO，往设备层写入数据。

第二种是缓存 I/O。最终我们会将数据从应用拷贝到内存缓存中，但是这个时候，并不执行真正的 I/O 操作。它们只将整个页或其中部分标记为脏。写操作由一个 timer 触发，那个时候，才调用 wb_workfn 往硬盘写入页面。

接下来的调用链为：wb\_workfn->wb\_do\_writeback->wb\_writeback->writeback\_sb\_inodes->\_\_writeback\_single\_inode->do\_writepages。在 do\_writepages 中，我们要调用 mapping->a\_ops->writepages，但实际调用的是 ext4_writepages，往设备层写入数据。

这一节，我们就沿着这两种情况分析下去。

## 直接 I/O 如何访问块设备？

我们先来看第一种情况，直接 I/O 调用到 ext4\_direct\_IO。

static 

 ssize_t 

 ext4\_direct\_IO(struct kiocb *iocb, struct iov_iter *iter)

{

struct 

 file *file = iocb->ki_filp;

struct 

 inode *inode = file->f_mapping->host;

size_t count = iov\_iter\_count(iter);

loff_t offset = iocb->ki_pos;

ssize_t ret;

......

ret = ext4\_direct\_IO_write(iocb, iter);

......

}

static 

 ssize_t 

 ext4\_direct\_IO_write(struct kiocb *iocb, struct iov_iter *iter)

{

struct 

 file *file = iocb->ki_filp;

struct 

 inode *inode = file->f_mapping->host;

struct 

 ext4\_inode\_info *ei = EXT4_I(inode);

ssize_t ret;

loff_t offset = iocb->ki_pos;

size_t count = iov\_iter\_count(iter);

......

ret = \_\_blockdev\_direct\_IO(iocb, inode, inode->i\_sb->s_bdev, iter,

get\_block\_func, ext4\_end\_io_dio, NULL,

dio_flags);

……

}

在 ext4\_direct\_IO\_write 调用 \_\_blockdev\_direct\_IO，有个参数你需要特别注意一下，那就是 inode->i\_sb->s\_bdev。通过当前文件的 inode，我们可以得到 super\_block。这个 super\_block 中的 s\_bdev，就是咱们上一节填进去的那个 block\_device。

\_\_blockdev\_direct\_IO 会调用 do\_blockdev\_direct\_IO，在这里面我们要准备一个 struct dio 结构和 struct dio_submit 结构，用来描述将要发生的写入请求。

static 

 inline 

 ssize_t

do\_blockdev\_direct_IO(struct kiocb *iocb, struct inode *inode,

struct block_device *bdev, struct iov_iter *iter,

get\_block\_t get_block, dio\_iodone\_t end_io,

dio\_submit\_t submit_io, int flags)

{

unsigned i_blkbits = ACCESS_ONCE(inode->i_blkbits);

unsigned blkbits = i_blkbits;

unsigned blocksize_mask = (1 << blkbits) - 1;

ssize_t retval = -EINVAL;

size_t count = iov\_iter\_count(iter);

loff_t offset = iocb->ki_pos;

loff_t end = offset + count;

struct 

 dio *dio;

struct 

 dio_submit sdio = { 0, };

struct 

 buffer_head map_bh = { 0, };

......

dio = kmem\_cache\_alloc(dio\_cache, GFP\_KERNEL);

dio->flags = flags;

dio->i_size = i\_size\_read(inode);

dio->inode = inode;

if (iov\_iter\_rw(iter) == WRITE) {

dio->op = REQ\_OP\_WRITE;

dio->op\_flags = REQ\_SYNC | REQ_IDLE;

if (iocb->ki\_flags & IOCB\_NOWAIT)

dio->op\_flags |= REQ\_NOWAIT;

} else {

dio->op = REQ\_OP\_READ;

}

sdio.blkbits = blkbits;

sdio.blkfactor = i_blkbits - blkbits;

sdio.block\_in\_file = offset >> blkbits;

sdio.get\_block = get\_block;

dio->end\_io = end\_io;

sdio.submit\_io = submit\_io;

sdio.final\_block\_in_bio = -1;

sdio.next\_block\_for_io = -1;

dio->iocb = iocb;

dio->refcount = 1;

sdio.iter = iter;

sdio.final\_block\_in_request =

(offset + iov\_iter\_count(iter)) >> blkbits;

......

sdio.pages\_in\_io += iov\_iter\_npages(iter, INT_MAX);

retval = do\_direct\_IO(dio, &sdio, &map_bh);

.....

}

do\_direct\_IO 里面有两层循环，第一层循环是依次处理这次要写入的所有块。对于每一块，取出对应的内存中的页 page，在这一块中，有写入的起始地址 from 和终止地址 to，所以，第二层循环就是依次处理 from 到 to 的数据，调用 submit\_page\_section，提交到块设备层进行写入。

static int do\_direct\_IO(struct 

 dio *dio, struct 

 dio_submit *sdio,

struct 

 buffer_head *map_bh)

{

const unsigned blkbits = sdio->blkbits;

const unsigned i_blkbits = blkbits + sdio->blkfactor;

int ret = 0;

while (sdio->block\_in\_file < sdio->final\_block\_in_request) {

struct 

 page *page;

size_t from, to;

page = dio\_get\_page(dio, sdio);

from = sdio->head ? 0 : sdio->from;

to = (sdio->head == sdio->tail - 1) ? sdio->to : PAGE_SIZE;

sdio->head++;

while (from < to) {

unsigned this\_chunk\_bytes;

unsigned this\_chunk\_blocks;

......

ret = submit\_page\_section(dio, sdio, page,

from,

this\_chunk\_bytes,

sdio->next\_block\_for_io,

map_bh);

......

sdio->next\_block\_for\_io += this\_chunk_blocks;

sdio->block\_in\_file += this\_chunk\_blocks;

from += this\_chunk\_bytes;

dio->result += this\_chunk\_bytes;

sdio->blocks\_available -= this\_chunk_blocks;

if (sdio->block\_in\_file == sdio->final\_block\_in_request)

break;

......

}

}

}

submit\_page\_section 会调用 dio\_bio\_submit，进而调用 submit_bio 向块设备层提交数据。其中，参数 struct bio 是将数据传给块设备的通用传输对象。定义如下：

\* submit_bio - submit a bio to the block device layer for I/O

\* @bio: The &struct bio which describes the I/O

*/

blk\_qc\_t 

 submit_bio(struct bio *bio)

{

......

return 

 generic\_make\_request(bio);

}

## 缓存 I/O 如何访问块设备？

我们再来看第二种情况，缓存 I/O 调用到 ext4_writepages。这个函数比较长，我们这里只截取最重要的部分来讲解。

static int ext4_writepages(struct 

 address_space *mapping,

struct 

 writeback_control *wbc)

{

......

struct 

 mpage\_da\_data mpd;

struct 

 inode *inode = mapping->host;

struct 

 ext4\_sb\_info *sbi = EXT4_SB(mapping->host->i_sb);

......

mpd.do_map = 0;

mpd.io\_submit.io\_end = ext4\_init\_io_end(inode, GFP_KERNEL);

ret = mpage\_prepare\_extent\_to\_map(&mpd);

ext4\_io\_submit(&mpd.io_submit);

......

}

这里比较重要的一个数据结构是 struct mpage\_da\_data。这里面有文件的 inode、要写入的页的偏移量，还有一个重要的 struct ext4\_io\_submit，里面有通用传输对象 bio。

struct 

 mpage\_da\_data {

struct 

 inode *inode;

......

pgoff_t first_page;

pgoff_t next_page;

pgoff_t last_page;

struct 

 ext4\_map\_blocks map;

struct 

 ext4\_io\_submit io_submit;

unsigned 

 int do_map:1;

};

struct 

 ext4\_io\_submit {

......

struct 

 bio *io_bio;

ext4\_io\_end_t *io_end;

sector_t io\_next\_block;

};

在 ext4\_writepages 中，mpage\_prepare\_extent\_to\_map 用于初始化这个 struct mpage\_da\_data 结构。接下来的调用链为：mpage\_prepare\_extent\_to\_map->mpage\_process\_page\_bufs->mpage\_submit\_page->ext4\_bio\_write\_page->io\_submit\_add\_bh。

在 io\_submit\_add\_bh 中，此时的 bio 还是空的，因而我们要调用 io\_submit\_init\_bio，初始化 bio。

static int io\_submit\_init_bio(struct 

 ext4\_io\_submit *io,

struct 

 buffer_head *bh)

{

struct 

 bio *bio;

bio = bio_alloc(GFP\_NOIO, BIO\_MAX_PAGES);

if (!bio)

return -ENOMEM;

wbc\_init\_bio(io->io_wbc, bio);

bio->bi\_iter.bi\_sector = bh->b_blocknr * (bh->b_size >> 9);

bio->bi_bdev = bh->b_bdev;

bio->bi\_end\_io = ext4\_end\_bio;

bio->bi_private = ext4\_get\_io_end(io->io_end);

io->io_bio = bio;

io->io\_next\_block = bh->b_blocknr;

return 

 0;

}

我们再回到 ext4\_writepages 中。在 bio 初始化完之后，我们要调用 ext4\_io\_submit，提交 I/O。在这里我们又是调用 submit\_bio，向块设备层传输数据。ext4\_io\_submit 的实现如下：

void ext4\_io\_submit(struct 

 ext4\_io\_submit *io)

{

struct 

 bio *bio = io->io_bio;

if (bio) {

int io\_op\_flags = io->io_wbc->sync\_mode == WB\_SYNC_ALL ?

REQ_SYNC : 0;

io->io_bio->bi\_write\_hint = io->io_end->inode->i\_write\_hint;

bio\_set\_op_attrs(io->io\_bio, REQ\_OP\_WRITE, io\_op_flags);

submit_bio(io->io_bio);

}

io->io_bio = NULL;

}

## 如何向块设备层提交请求？

既然不管是直接 I/O，还是缓存 I/O，最后都到了 submit_bio 里面，那我们就来重点分析一下它。

submit\_bio 会调用 generic\_make_request。代码如下：

blk\_qc\_t 

 generic\_make\_request(struct bio *bio)

{

\* bio\_list\_on_stack\[0\] contains bios submitted by the current

\* make\_request\_fn.

\* bio\_list\_on_stack\[1\] contains bios that were submitted before

\* the current make\_request\_fn, but that haven't been processed

\* yet.

*/

struct 

 bio_list bio\_list\_on_stack\[2\];

blk\_qc\_t ret = BLK\_QC\_T_NONE;

......

if (current->bio_list) {

bio\_list\_add(&current->bio_list\[0\], bio);

goto out;

}

bio\_list\_init(&bio\_list\_on_stack\[0\]);

current->bio\_list = bio\_list\_on\_stack;

do {

struct 

 request_queue *q = bdev\_get\_queue(bio->bi_bdev);

if (likely(blk\_queue\_enter(q, bio->bi\_opf & REQ\_NOWAIT) == 0)) {

struct 

 bio_list lower, same;

bio\_list\_on_stack\[1\] = bio\_list\_on_stack\[0\];

bio\_list\_init(&bio\_list\_on_stack\[0\]);

ret = q->make\_request\_fn(q, bio);

blk\_queue\_exit(q);

\* and those for the same level

*/

bio\_list\_init(&lower);

bio\_list\_init(&same);

while ((bio = bio\_list\_pop(&bio\_list\_on_stack\[0\])) != NULL)

if (q == bdev\_get\_queue(bio->bi_bdev))

bio\_list\_add(&same, bio);

else

bio\_list\_add(&lower, bio);

bio\_list\_merge(&bio\_list\_on_stack\[0\], &lower);

bio\_list\_merge(&bio\_list\_on_stack\[0\], &same);

bio\_list\_merge(&bio\_list\_on_stack\[0\], &bio\_list\_on_stack\[1\]);

}

......

bio = bio\_list\_pop(&bio\_list\_on_stack\[0\]);

} while (bio);

current->bio_list = NULL;

out:

return ret;

}

这里的逻辑有点复杂，我们先来看大的逻辑。在 do-while 中，我们先是获取一个请求队列 request\_queue，然后调用这个队列的 make\_request_fn 函数。

### 块设备队列结构

如果再来看 struct block\_device 结构和 struct gendisk 结构，我们会发现，每个块设备都有一个请求队列 struct request\_queue，用于处理上层发来的请求。

在每个块设备的驱动程序初始化的时候，会生成一个 request_queue。

struct 

 request_queue {

\* Together with queue_head for cacheline sharing

*/

struct 

 list_head queue_head;

struct 

 request *last_merge;

struct 

 elevator_queue *elevator;

......

request\_fn\_proc *request_fn;

make\_request\_fn *make\_request\_fn;

......

}

在请求队列 request\_queue 上，首先是有一个链表 list\_head，保存请求 request。

struct 

 request {

struct 

 list_head queuelist;

......

struct 

 request_queue *q;

......

struct 

 bio *bio;

struct 

 bio *biotail;

......

}

每个 request 包括一个链表的 struct bio，有指针指向一头一尾。

struct 

 bio {

struct 

 bio *bi_next;

struct 

 block_device *bi_bdev;

blk\_status\_t bi_status;

......

struct 

 bvec_iter bi_iter;

unsigned 

 short bi_vcnt;

unsigned 

 short bi\_max\_vecs;

atomic_t \_\_bi\_cnt;

struct 

 bio_vec *bi\_io\_vec;

......

};

struct 

 bio_vec {

struct 

 page *bv_page;

unsigned 

 int bv_len;

unsigned 

 int bv_offset;

}

在 bio 中，bi\_next 是链表中的下一项，struct bio\_vec 指向一组页面。

![[3c473d163b6e90985d7301f115ab660e_df0f8167a90242cd8.jpg]]

在请求队列 request\_queue 上，还有两个重要的函数，一个是 make\_request\_fn 函数，用于生成 request；另一个是 request\_fn 函数，用于处理 request。

### 块设备的初始化

我们还是以 scsi 驱动为例。在初始化设备驱动的时候，我们会调用 scsi\_alloc\_queue，把 request\_fn 设置为 scsi\_request\_fn。我们还会调用 blk\_init\_allocated\_queue->blk\_queue\_make\_request，把 make\_request\_fn 设置为 blk\_queue_bio。

\* scsi\_alloc\_sdev - allocate and setup a scsi_Device

\* @starget: which target to allocate a &scsi_device for

\* @lun: which lun

\* @hostdata: usually NULL and set by ->slave_alloc instead

*

\* Description:

\* Allocate, initialize for io, and return a pointer to a scsi_Device.

\* Stores the @shost, @channel, @id, and @lun in the scsi_Device, and

\* adds scsi_Device to the appropriate list.

*

\* Return value:

\* scsi_Device pointer, or NULL on failure.

**/

static 

 struct 

 scsi_device *scsi\_alloc\_sdev(struct 

 scsi_target *starget,

u64 lun, void *hostdata)

{

struct 

 scsi_device *sdev;

sdev = kzalloc(sizeof(*sdev) + shost->transportt->device_size,

GFP_ATOMIC);

......

sdev->request_queue = scsi\_alloc\_queue(sdev);

......

}

struct 

 request_queue *scsi\_alloc\_queue(struct 

 scsi_device *sdev)

{

struct 

 Scsi_Host *shost = sdev->host;

struct 

 request_queue *q;

q = blk\_alloc\_queue_node(GFP\_KERNEL, NUMA\_NO_NODE);

if (!q)

return NULL;

q->cmd_size = sizeof(struct 

 scsi_cmnd) \+ shost->hostt->cmd_size;

q->rq\_alloc\_data = shost;

q->request\_fn = scsi\_request_fn;

q->init\_rq\_fn = scsi\_init\_rq;

q->exit\_rq\_fn = scsi\_exit\_rq;

q->initialize\_rq\_fn = scsi\_initialize\_rq;

if (blk\_init\_allocated_queue(q) < 0) {

blk\_cleanup\_queue(q);

return NULL;

}

\_\_scsi\_init_queue(shost, q);

......

return q

}

在 blk\_init\_allocated\_queue 中，除了初始化 make\_request_fn 函数，我们还要做一件很重要的事情，就是初始化 I/O 的电梯算法。

int 

 blk\_init\_allocated_queue(struct request_queue *q)

{

q->fq = blk\_alloc\_flush_queue(q, NUMA\_NO\_NODE, q->cmd_size);

......

blk\_queue\_make_request(q, blk\_queue\_bio);

......

if (elevator_init(q, NULL)) {

......

}

......

}

电梯算法有很多种类型，定义为 elevator_type。下面我来逐一说一下。

struct elevator\_type elevator\_noop

Noop 调度算法是最简单的 IO 调度算法，它将 IO 请求放入到一个 FIFO 队列中，然后逐个执行这些 IO 请求。

struct elevator\_type iosched\_deadline

Deadline 算法要保证每个 IO 请求在一定的时间内一定要被服务到，以此来避免某个请求饥饿。为了完成这个目标，算法中引入了两类队列，一类队列用来对请求按起始扇区序号进行排序，通过红黑树来组织，我们称为 sort\_list，按照此队列传输性能会比较高；另一类队列对请求按它们的生成时间进行排序，由链表来组织，称为 fifo\_list，并且每一个请求都有一个期限值。

struct elevator\_type iosched\_cfq

又看到了熟悉的 CFQ 完全公平调度算法。所有的请求会在多个队列中排序。同一个进程的请求，总是在同一队列中处理。时间片会分配到每个队列，通过轮询算法，我们保证了 I/O 带宽，以公平的方式，在不同队列之间进行共享。

elevator\_init 中会根据名称来指定电梯算法，如果没有选择，那就默认使用 iosched\_cfq。

### 请求提交与调度

接下来，我们回到 generic\_make\_request 函数中。调用队列的 make\_request\_fn 函数，其实就是调用 blk\_queue\_bio。

static 

 blk\_qc\_t 

 blk\_queue\_bio(struct request_queue *q, struct bio *bio)

{

struct 

 request \*req, \*free;

unsigned 

 int request_count = 0;

......

switch (elv_merge(q, &req, bio)) {

case ELEVATOR\_BACK\_MERGE:

if (!bio\_attempt\_back_merge(q, req, bio))

break;

elv\_bio\_merged(q, req, bio);

free = attempt\_back\_merge(q, req);

if (free)

\_\_blk\_put_request(q, free);

else

elv\_merged\_request(q, req, ELEVATOR\_BACK\_MERGE);

goto out_unlock;

case ELEVATOR\_FRONT\_MERGE:

if (!bio\_attempt\_front_merge(q, req, bio))

break;

elv\_bio\_merged(q, req, bio);

free = attempt\_front\_merge(q, req);

if (free)

\_\_blk\_put_request(q, free);

else

elv\_merged\_request(q, req, ELEVATOR\_FRONT\_MERGE);

goto out_unlock;

default:

break;

}

get_rq:

req = get_request(q, bio->bi\_opf, bio, GFP\_NOIO);

......

blk\_init\_request\_from\_bio(req, bio);

......

add\_acct\_request(q, req, where);

\_\_blk\_run_queue(q);

out_unlock:

......

return BLK\_QC\_T_NONE;

}

blk\_queue\_bio 首先做的一件事情是调用 elv_merge 来判断，当前这个 bio 请求是否能够和目前已有的 request 合并起来，成为同一批 I/O 操作，从而提高读取和写入的性能。

判断标准和 struct bio 的成员 struct bvec\_iter 有关，它里面有两个变量，一个是起始磁盘簇 bi\_sector，另一个是大小 bi_size。

enum 

 elv_merge 

 elv_merge(struct 

 request_queue *q, struct 

 request **req,

struct 

 bio *bio)

{

struct 

 elevator_queue *e = q->elevator;

struct 

 request *__rq;

......

if (q->last_merge && elv\_bio\_merge_ok(q->last_merge, bio)) {

enum 

 elv_merge ret = blk\_try\_merge(q->last_merge, bio);

if (ret != ELEVATOR\_NO\_MERGE) {

*req = q->last_merge;

return ret;

}

}

......

__rq = elv\_rqhash\_find(q, bio->bi\_iter.bi\_sector);

if (__rq && elv\_bio\_merge_ok(__rq, bio)) {

*req = __rq;

return ELEVATOR\_BACK\_MERGE;

}

if (e->uses_mq && e->type->ops.mq.request_merge)

return e->type->ops.mq.request_merge(q, req, bio);

else 

 if (!e->uses_mq && e->type->ops.sq.elevator\_merge\_fn)

return e->type->ops.sq.elevator\_merge\_fn(q, req, bio);

return ELEVATOR\_NO\_MERGE;

}

elv_merge 尝试了三次合并。

第一次，它先判断和上一次合并的 request 能不能再次合并，看看能不能赶上马上要走的这部电梯。在 blk\_try\_merge 主要做了这样的判断：如果 blk\_rq\_pos(rq) + blk\_rq\_sectors(rq) == bio->bi\_iter.bi\_sector，也就是说这个 request 的起始地址加上它的大小（其实是这个 request 的结束地址），如果和 bio 的起始地址能接得上，那就把 bio 放在 request 的最后，我们称为 ELEVATOR\_BACK\_MERGE。

如果 blk\_rq\_pos(rq) - bio\_sectors(bio) == bio->bi\_iter.bi\_sector，也就是说，这个 request 的起始地址减去 bio 的大小等于 bio 的起始地址，这说明 bio 放在 request 的最前面能够接得上，那就把 bio 放在 request 的最前面，我们称为 ELEVATOR\_FRONT\_MERGE。否则，那就不合并，我们称为 ELEVATOR\_NO_MERGE。

enum 

 elv_merge 

 blk\_try\_merge(struct 

 request *rq, struct 

 bio *bio)

{

......

if (blk\_rq\_pos(rq) + blk\_rq\_sectors(rq) == bio->bi\_iter.bi\_sector)

return ELEVATOR\_BACK\_MERGE;

else 

 if (blk\_rq\_pos(rq) - bio_sectors(bio) == bio->bi\_iter.bi\_sector)

return ELEVATOR\_FRONT\_MERGE;

return ELEVATOR\_NO\_MERGE;

}

第二次，如果和上一个合并过的 request 无法合并，那我们就调用 elv\_rqhash\_find。然后按照 bio 的起始地址查找 request，看有没有能够合并的。如果有的话，因为是按照起始地址找的，应该接在人家的后面，所以是 ELEVATOR\_BACK\_MERGE。

第三次，调用 elevator\_merge\_fn 试图合并。对于 iosched\_cfq，调用的是 cfq\_merge。在这里面，cfq\_find\_rq\_fmerge 会调用 elv\_rb\_find 函数，里面的参数是 bio 的结束地址。我们还是要看，能不能找到可以合并的。如果有的话，因为是按照结束地址找的，应该接在人家前面，所以是 ELEVATOR\_FRONT_MERGE。

static 

 enum 

 elv_merge 

 cfq_merge(struct 

 request_queue *q, struct 

 request **req,

struct 

 bio *bio)

{

struct 

 cfq_data *cfqd = q->elevator->elevator_data;

struct 

 request *__rq;

__rq = cfq\_find\_rq_fmerge(cfqd, bio);

if (__rq && elv\_bio\_merge_ok(__rq, bio)) {

*req = __rq;

return ELEVATOR\_FRONT\_MERGE;

}

return ELEVATOR\_NO\_MERGE;

}

static 

 struct 

 request *

cfq\_find\_rq_fmerge(struct 

 cfq_data *cfqd, struct 

 bio *bio)

{

struct 

 task_struct *tsk = current;

struct 

 cfq\_io\_cq *cic;

struct 

 cfq_queue *cfqq;

cic = cfq\_cic\_lookup(cfqd, tsk->io_context);

if (!cic)

return NULL;

cfqq = cic\_to\_cfqq(cic, op\_is\_sync(bio->bi_opf));

if (cfqq)

return 

 elv\_rb\_find(&cfqq->sort_list, bio\_end\_sector(bio));

return NUL

}

等从 elv\_merge 返回 blk\_queue\_bio 的时候，我们就知道，应该做哪种类型的合并，接着就要进行真的合并。如果没有办法合并，那就调用 get\_request，创建一个新的 request，调用 blk\_init\_request\_from\_bio，将 bio 放到新的 request 里面，然后调用 add\_acct\_request，把新的 request 加到 request_queue 队列中。

至此，我们解析完了 generic\_make\_request 中最重要的两大逻辑：获取一个请求队列 request\_queue 和调用这个队列的 make\_request_fn 函数。

其实，generic\_make\_request 其他部分也很令人困惑。感觉里面有特别多的 struct bio_list，倒腾过来，倒腾过去的。这是因为，很多块设备是有层次的。

比如，我们用两块硬盘组成 RAID，两个 RAID 盘组成 LVM，然后我们就可以在 LVM 上创建一个块设备给用户用，我们称接近用户的块设备为高层次的块设备，接近底层的块设备为低层次（lower）的块设备。这样，generic\_make\_request 把 I/O 请求发送给高层次的块设备的时候，会调用高层块设备的 make\_request\_fn，高层块设备又要调用 generic\_make\_request，将请求发送给低层次的块设备。虽然块设备的层次不会太多，但是对于代码 generic\_make\_request 来讲，这可是递归的调用，一不小心，就会递归过深，无法正常退出，而且内核栈的大小又非常有限，所以要比较小心。

这里你是否理解了 struct bio\_list bio\_list\_on\_stack\[2\]的名字为什么叫 stack 呢？其实，将栈的操作变成对于队列的操作，队列不在栈里面，会大很多。每次 generic\_make\_request 被当前任务调用的时候，将 current->bio\_list 设置为 bio\_list\_on\_stack，并在 generic\_make\_request 的一开始就判断 current->bio\_list 是否为空。如果不为空，说明已经在 generic\_make\_request 的调用里面了，就不必调用 make\_request\_fn 进行递归了，直接把请求加入到 bio\_list 里面就可以了，这就实现了递归的及时退出。

如果 current->bio\_list 为空，那我们就将 current->bio\_list 设置为 bio\_list\_on\_stack 后，进入 do-while 循环，做咱们分析过的 generic\_make\_request 的两大逻辑。但是，当前的队列调用 make\_request\_fn 的时候，在 make\_request\_fn 的具体实现中，会生成新的 bio。调用更底层的块设备，也会生成新的 bio，都会放在 bio\_list\_on\_stack 的队列中，是一个边处理还边创建的过程。

bio\_list\_on\_stack\[1\] = bio\_list\_on\_stack\[0\]这一句在 make\_request\_fn 之前，将之前队列里面遗留没有处理的保存下来，接着 bio\_list\_init 将 bio\_list\_on\_stack\[0\]设置为空，然后调用 make\_request\_fn，在 make\_request\_fn 里面如果有新的 bio 生成，都会加到 bio\_list\_on\_stack\[0\]这个队列里面来。

make\_request\_fn 执行完毕后，可以想象 bio\_list\_on\_stack\[0\]可能又多了一些 bio 了，接下来的循环中调用 bio\_list\_pop 将 bio\_list\_on\_stack\[0\]积攒的 bio 拿出来，分别放在两个队列 lower 和 same 中，顾名思义，lower 就是更低层次的块设备的 bio，same 是同层次的块设备的 bio。

接下来我们能将 lower、same 以及 bio\_list\_on\_stack\[1\] 都取出来，放在 bio\_list\_on\_stack\[0\]统一进行处理。当然应该 lower 优先了，因为只有底层的块设备的 I/O 做完了，上层的块设备的 I/O 才能做完。

到这里，generic\_make\_request 的逻辑才算解析完毕。对于写入的数据来讲，其实仅仅是将 bio 请求放在请求队列上，设备驱动程序还没往设备里面写呢。

### 请求的处理

设备驱动程序往设备里面写，调用的是请求队列 request\_queue 的另外一个函数 request\_fn。对于 scsi 设备来讲，调用的是 scsi\_request\_fn。

static void scsi\_request\_fn(struct 

 request_queue *q)

__releases(q->queue_lock)

__acquires(q->queue_lock)

{

struct 

 scsi_device *sdev = q->queuedata;

struct 

 Scsi_Host *shost;

struct 

 scsi_cmnd *cmd;

struct 

 request *req;

\* To start with, we keep looping until the queue is empty, or until

\* the host is no longer able to accept any more requests.

*/

shost = sdev->host;

for (;;) {

int rtn;

\* get next queueable request. We do this early to make sure

\* that the request is fully prepared even if we cannot

\* accept it.

*/

req = blk\_peek\_request(q);

......

\* Remove the request from the request list.

*/

if (!(blk\_queue\_tagged(q) && !blk\_queue\_start_tag(q, req)))

blk\_start\_request(req);

.....

cmd = req->special;

......

\* Dispatch the command to the low-level driver.

*/

cmd->scsi\_done = scsi\_done;

rtn = scsi\_dispatch\_cmd(cmd);

......

}

return;

......

}

在这里面是一个 for 无限循环，从 request_queue 中读取 request，然后封装更加底层的指令，给设备控制器下指令，实施真正的 I/O 操作。

## 总结时刻

这一节我们讲了如何将块设备 I/O 请求送达到外部设备。

对于块设备的 I/O 操作分为两种，一种是直接 I/O，另一种是缓存 I/O。无论是哪种 I/O，最终都会调用 submit_bio 提交块设备 I/O 请求。

对于每一种块设备，都有一个 gendisk 表示这个设备，它有一个请求队列，这个队列是一系列的 request 对象。每个 request 对象里面包含多个 BIO 对象，指向 page cache。所谓的写入块设备，I/O 就是将 page cache 里面的数据写入硬盘。

对于请求队列来讲，还有两个函数，一个函数叫 make\_request\_fn 函数，用于将请求放入队列。submit\_bio 会调用 generic\_make_request，然后调用这个函数。

另一个函数往往在设备驱动程序里实现，我们叫 request_fn 函数，它用于从队列里面取出请求来，写入外部设备。

![[c9f6a08075ba4eae3314523fa258363c_96864e4a327842e1b.png]]

至此，整个写入文件的过程才算完全结束。这真是个复杂的过程，涉及系统调用、内存管理、文件系统和输入输出。这足以说明，操作系统真的是一个非常复杂的体系，环环相扣，需要分层次层层展开来学习。

到这里，专栏已经过半了，你应该能发现，很多我之前说“后面会细讲”的东西，现在正在一点一点解释清楚，而文中越来越多出现“前面我们讲过”的字眼，你是否当时学习前面知识的时候，没有在意，导致学习后面的知识产生困惑了呢？没关系，及时倒回去复习，再回过头去看，当初学过的很多知识会变得清晰很多。

## 课堂练习

你知道如何查看磁盘调度算法、修改磁盘调度算法以及 I/O 队列的长度吗？

欢迎留言和我分享你的疑惑和见解 ，也欢迎可以收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。

![[8c0a95fa07a8b9a1abfd394479bdd637_627a10fe47f449c68.jpg]]