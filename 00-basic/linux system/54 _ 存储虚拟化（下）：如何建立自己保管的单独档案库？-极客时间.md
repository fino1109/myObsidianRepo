上一节，我们讲了 qemu 启动过程中的存储虚拟化。好了，现在 qemu 启动了，硬盘设备文件已经打开了。那如果我们要往虚拟机的一个进程写入一个文件，该怎么做呢？最终这个文件又是如何落到宿主机上的硬盘文件的呢？这一节，我们一起来看一看。

## 前端设备驱动 virtio_blk

虚拟机里面的进程写入一个文件，当然要通过文件系统。整个过程和咱们在文件系统那一节讲的过程没有区别。只是到了设备驱动层，我们看到的就不是普通的硬盘驱动了，而是 virtio 的驱动。

virtio 的驱动程序代码在 Linux 操作系统的源代码里面，文件名叫 drivers/block/virtio_blk.c。

static 

 int __init init(void)

{

int error;

virtblk\_wq = alloc\_workqueue("virtio-blk", 0, 0);

major = register_blkdev(0, "virtblk");

error = register\_virtio\_driver(&virtio_blk);

......

}

module_init(init);

module_exit(fini);

MODULE\_DEVICE\_TABLE(virtio, id_table);

MODULE_DESCRIPTION("Virtio block driver");

MODULE_LICENSE("GPL");

static struct virtio_driver 

 virtio_blk 

 = {

......

.driver.name = KBUILD_MODNAME,

.driver.owner = THIS_MODULE,

.id\_table = id\_table,

.probe = virtblk_probe,

.remove = virtblk_remove,

......

};

前面我们介绍过设备驱动程序，从这里的代码中，我们能看到非常熟悉的结构。它会创建一个 workqueue，注册一个块设备，并获得一个主设备号，然后注册一个驱动函数 virtio_blk。

当一个设备驱动作为一个内核模块被初始化的时候，probe 函数会被调用，因而我们来看一下 virtblk_probe。

static int virtblk_probe(struct 

 virtio_device *vdev)

{

struct 

 virtio_blk *vblk;

struct 

 request_queue *q;

......

vdev->priv = vblk = kmalloc(sizeof(*vblk), GFP_KERNEL);

vblk->vdev = vdev;

vblk->sg\_elems = sg\_elems;

INIT_WORK(&vblk->config\_work, virtblk\_config\_changed\_work);

......

err = init_vq(vblk);

......

vblk->disk = alloc_disk(1 << PART_BITS);

memset(&vblk->tag_set, 0, sizeof(vblk->tag_set));

vblk->tag\_set.ops = &virtio\_mq_ops;

vblk->tag\_set.queue\_depth = virtblk\_queue\_depth;

vblk->tag\_set.numa\_node = NUMA\_NO\_NODE;

vblk->tag\_set.flags = BLK\_MQ\_F\_SHOULD_MERGE;

vblk->tag\_set.cmd\_size =

sizeof(struct 

 virtblk_req) +

sizeof(struct 

 scatterlist) \* sg_elems;

vblk->tag\_set.driver\_data = vblk;

vblk->tag\_set.nr\_hw_queues = vblk->num_vqs;

err = blk\_mq\_alloc\_tag\_set(&vblk->tag_set);

......

q = blk\_mq\_init_queue(&vblk->tag_set);

vblk->disk->queue = q;

q->queuedata = vblk;

virtblk\_name\_format("vd", index, vblk->disk->disk\_name, DISK\_NAME_LEN);

vblk->disk->major = major;

vblk->disk->first_minor = index\_to\_minor(index);

vblk->disk->private_data = vblk;

vblk->disk->fops = &virtblk_fops;

vblk->disk->flags |= GENHD\_FL\_EXT_DEVT;

vblk->index = index;

......

device\_add\_disk(&vdev->dev, vblk->disk);

err = device\_create\_file(disk\_to\_dev(vblk->disk), &dev\_attr\_serial);

......

}

在 virtblk\_probe 中，我们首先看到的是 struct request\_queue，这是每一个块设备都有的一个队列。还记得吗？它有两个函数，一个是 make\_request\_fn 函数，用于生成 request；另一个是 request_fn 函数，用于处理 request。

这个 request\_queue 的初始化过程在 blk\_mq\_init\_queue 中。它会调用 blk\_mq\_init\_allocated\_queue->blk\_queue\_make\_request。在这里面，我们可以将 make\_request\_fn 函数设置为 blk\_mq\_make\_request，也就是说，一旦上层有写入请求，我们就通过 blk\_mq\_make\_request 这个函数，将请求放入 request\_queue 队列中。

另外，在 virtblk_probe 中，我们会初始化一个 gendisk。前面我们也讲了，每一个块设备都有这样一个结构。

在 virtblk\_probe 中，还有一件重要的事情就是，init\_vq 会来初始化 virtqueue。

static 

 int init_vq(struct virtio_blk *vblk)

{

int err;

int i;

vq\_callback\_t **callbacks;

const 

 char **names;

struct virtqueue **vqs;

unsigned 

 short num_vqs;

struct virtio_device *vdev = vblk->vdev;

......

vblk->vqs = kmalloc\_array(num\_vqs, sizeof(*vblk->vqs), GFP_KERNEL);

names = kmalloc\_array(num\_vqs, sizeof(*names), GFP_KERNEL);

callbacks = kmalloc\_array(num\_vqs, sizeof(*callbacks), GFP_KERNEL);

vqs = kmalloc\_array(num\_vqs, sizeof(*vqs), GFP_KERNEL);

......

for (i = 0; i < num_vqs; i++) {

callbacks\[i\] = virtblk_done;

names\[i\] = vblk->vqs\[i\].name;

}

err = virtio\_find\_vqs(vdev, num_vqs, vqs, callbacks, names, &desc);

for (i = 0; i < num_vqs; i++) {

vblk->vqs\[i\].vq = vqs\[i\];

}

vblk->num\_vqs = num\_vqs;

......

}

按照上面的原理来说，virtqueue 是一个介于客户机前端和 qemu 后端的一个结构，用于在这两端之间传递数据。这里建立的 struct virtqueue 是客户机前端对于队列的管理的数据结构，在客户机的 linux 内核中通过 kmalloc_array 进行分配。

而队列的实体需要通过函数 virtio\_find\_vqs 查找或者生成，所以这里我们还把 callback 函数指定为 virtblk_done。当 buffer 使用发生变化的时候，我们需要调用这个 callback 函数进行通知。

static 

 inline

int 

 virtio\_find\_vqs(struct virtio_device *vdev, unsigned nvqs,

struct virtqueue *vqs\[\], vq\_callback\_t *callbacks\[\],

const 

 char \* const names\[\],

struct irq_affinity *desc)

{

return vdev->config->find_vqs(vdev, nvqs, vqs, callbacks, names, NULL, desc);

}

static 

 const 

 struct 

 virtio\_config\_ops virtio\_pci\_config_ops = {

.get = vp_get,

.set = vp_set,

.generation = vp_generation,

.get\_status = vp\_get_status,

.set\_status = vp\_set_status,

.reset = vp_reset,

.find\_vqs = vp\_modern\_find\_vqs,

.del\_vqs = vp\_del_vqs,

.get\_features = vp\_get_features,

.finalize\_features = vp\_finalize_features,

.bus\_name = vp\_bus_name,

.set\_vq\_affinity = vp\_set\_vq_affinity,

.get\_vq\_affinity = vp\_get\_vq_affinity,

};

根据 virtio\_config\_ops 的定义，virtio\_find\_vqs 会调用 vp\_modern\_find_vqs。

static int vp\_modern\_find_vqs(struct 

 virtio_device *vdev, unsigned nvqs,

struct 

 virtqueue *vqs\[\],

vq\_callback\_t *callbacks\[\],

const 

 char \* const names\[\], const 

 bool *ctx,

struct 

 irq_affinity *desc)

{

struct 

 virtio\_pci\_device *vp_dev = to\_vp\_device(vdev);

struct 

 virtqueue *vq;

int rc = vp\_find\_vqs(vdev, nvqs, vqs, callbacks, names, ctx, desc);

\* this, there's no way to go back except reset.

*/

list\_for\_each_entry(vq, &vdev->vqs, list) {

vp_iowrite16(vq->index, &vp_dev->common->queue_select);

vp_iowrite16(1, &vp_dev->common->queue_enable);

}

return 

 0;

}

在 vp\_modern\_find\_vqs 中，vp\_find\_vqs 会调用 vp\_find\_vqs\_intx。

static int vp\_find\_vqs_intx(struct 

 virtio_device *vdev, unsigned nvqs,

struct 

 virtqueue \*vqs\[\], vq\_callback\_t \*callbacks\[\],

const 

 char \* const names\[\], const 

 bool *ctx)

{

struct 

 virtio\_pci\_device *vp_dev = to\_vp\_device(vdev);

int i, err;

vp_dev->vqs = kcalloc(nvqs, sizeof(*vp_dev->vqs), GFP_KERNEL);

err = request_irq(vp_dev->pci_dev->irq, vp\_interrupt, IRQF\_SHARED,

dev_name(&vdev->dev), vp_dev);

vp_dev->intx_enabled = 1;

vp_dev->per\_vq\_vectors = false;

for (i = 0; i < nvqs; ++i) {

vqs\[i\] = vp\_setup\_vq(vdev, i, callbacks\[i\], names\[i\],

ctx ? ctx\[i\] : false,

VIRTIO\_MSI\_NO_VECTOR);

......

}

}

在 vp\_find\_vqs\_intx 中，我们通过 request\_irq 注册一个中断处理函数 vp_interrupt，当设备的配置信息发生改变，会产生一个中断，当设备向队列中写入信息时，也会产生一个中断，我们称为 vq 中断，中断处理函数需要调用相应的队列的回调函数。

然后，我们根据队列的数目，依次调用 vp\_setup\_vq，完成 virtqueue、vring 的分配和初始化。

static 

 struct 

 virtqueue *vp\_setup\_vq(struct 

 virtio_device *vdev, unsigned index,

void (*callback)(struct 

 virtqueue *vq),

const 

 char *name,

bool ctx,

u16 msix_vec)

{

struct 

 virtio\_pci\_device *vp_dev = to\_vp\_device(vdev);

struct 

 virtio\_pci\_vq_info *info = kmalloc(sizeof *info, GFP_KERNEL);

struct 

 virtqueue *vq;

unsigned long flags;

......

vq = vp_dev->setup_vq(vp_dev, info, index, callback, name, ctx,

msix_vec);

info->vq = vq;

if (callback) {

spin\_lock\_irqsave(&vp_dev->lock, flags);

list_add(&info->node, &vp_dev->virtqueues);

spin\_unlock\_irqrestore(&vp_dev->lock, flags);

} else {

INIT\_LIST\_HEAD(&info->node);

}

vp_dev->vqs\[index\] = info;

return vq;

}

static 

 struct 

 virtqueue *setup_vq(struct 

 virtio\_pci\_device *vp_dev,

struct 

 virtio\_pci\_vq_info *info,

unsigned index,

void (*callback)(struct 

 virtqueue *vq),

const 

 char *name,

bool ctx,

u16 msix_vec)

{

struct 

 virtio\_pci\_common_cfg \_\_iomem *cfg = vp\_dev->common;

struct 

 virtqueue *vq;

u16 num, off;

int err;

vp_iowrite16(index, &cfg->queue_select);

num = vp_ioread16(&cfg->queue_size);

off = vp_ioread16(&cfg->queue\_notify\_off);

info->msix\_vector = msix\_vec;

vq = vring\_create\_virtqueue(index, num,

SMP\_CACHE\_BYTES, &vp_dev->vdev,

true, true, ctx,

vp_notify, callback, name);

vp_iowrite16(virtqueue\_get\_vring_size(vq), &cfg->queue_size);

vp\_iowrite64\_twopart(virtqueue\_get\_desc_addr(vq),

&cfg->queue\_desc\_lo, &cfg->queue\_desc\_hi);

vp\_iowrite64\_twopart(virtqueue\_get\_avail_addr(vq),

&cfg->queue\_avail\_lo, &cfg->queue\_avail\_hi);

vp\_iowrite64\_twopart(virtqueue\_get\_used_addr(vq),

&cfg->queue\_used\_lo, &cfg->queue\_used\_hi);

......

return vq;

}

struct 

 virtqueue *vring\_create\_virtqueue(

unsigned int index,

unsigned int num,

unsigned int vring_align,

struct 

 virtio_device *vdev,

bool weak_barriers,

bool may\_reduce\_num,

bool context,

bool (*notify)(struct 

 virtqueue *),

void (*callback)(struct 

 virtqueue *),

const 

 char *name)

{

struct 

 virtqueue *vq;

void *queue = NULL;

dma\_addr\_t dma_addr;

size\_t queue\_size\_in\_bytes;

struct 

 vring vring;

for (; num && vring_size(num, vring\_align) > PAGE\_SIZE; num /= 2) {

queue = vring\_alloc\_queue(vdev, vring_size(num, vring_align),

&dma_addr,

GFP\_KERNEL|\_\_GFP\_NOWARN|\_\_GFP_ZERO);

if (queue)

break;

}

if (!queue) {

queue = vring\_alloc\_queue(vdev, vring_size(num, vring_align),

&dma\_addr, GFP\_KERNEL|\_\_GFP\_ZERO);

}

queue\_size\_in_bytes = vring_size(num, vring_align);

vring_init(&vring, num, queue, vring_align);

vq = \_\_vring\_new\_virtqueue(index, vring, vdev, weak\_barriers, context, notify, callback, name);

to_vvq(vq)->queue\_dma\_addr = dma_addr;

to_vvq(vq)->queue\_size\_in\_bytes = queue\_size\_in\_bytes;

to_vvq(vq)->we\_own\_ring = true;

return vq;

}

在 vring\_create\_virtqueue 中，我们会调用 vring\_alloc\_queue，来创建队列所需要的内存空间，然后调用 vring\_init 初始化结构 struct vring，来管理队列的内存空间，调用 \_\_vring\_new\_virtqueue，来创建 struct vring_virtqueue。

这个结构的一开始，是 struct virtqueue，它也是 struct virtqueue 的一个扩展，紧接着后面就是 struct vring。

struct 

 vring_virtqueue {

struct 

 virtqueue vq;

struct 

 vring vring;

......

}

至此我们发现，虚拟机里面的 virtio 的前端是这样的结构：struct virtio\_device 里面有一个 struct vring\_virtqueue，在 struct vring_virtqueue 里面有一个 struct vring。

## 中间 virtio 队列的管理

还记不记得我们上面讲 qemu 初始化的时候，virtio 的后端有数据结构 VirtIODevice，VirtQueue 和 vring 一模一样，前端和后端对应起来，都应该指向刚才创建的那一段内存。

现在的问题是，我们刚才分配的内存在客户机的内核里面，如何告知 qemu 来访问这段内存呢？

别忘了，qemu 模拟出来的 virtio block device 只是一个 PCI 设备。对于客户机来讲，这是一个外部设备，我们可以通过给外部设备发送指令的方式告知外部设备，这就是代码中 vp_iowrite16 的作用。它会调用专门给外部设备发送指令的函数 iowrite，告诉外部的 PCI 设备。

告知的有三个地址 virtqueue\_get\_desc\_addr、virtqueue\_get\_avail\_addr，virtqueue\_get\_used_addr。从客户机角度来看，这里面的地址都是物理地址，也即 GPA（Guest Physical Address）。因为只有物理地址才是客户机和 qemu 程序都认可的地址，本来客户机的物理内存也是 qemu 模拟出来的。

在 qemu 中，对 PCI 总线添加一个设备的时候，我们会调用 virtio\_pci\_device_plugged。

static 

 void 

 virtio\_pci\_device_plugged(DeviceState *d, Error **errp)

{

VirtIOPCIProxy *proxy = VIRTIO_PCI(d);

......

memory\_region\_init_io(&proxy->bar, OBJECT(proxy),

&virtio\_pci\_config_ops,

proxy, "virtio-pci", size);

......

}

static 

 const 

 MemoryRegionOps virtio\_pci\_config_ops = {

.read = virtio\_pci\_config_read,

.write = virtio\_pci\_config_write,

.impl = {

.min\_access\_size = 1,

.max\_access\_size = 4,

},

.endianness = DEVICE\_LITTLE\_ENDIAN,

};

在这里面，对于这个加载的设备进行 I/O 操作，会映射到读写某一块内存空间，对应的操作为 virtio\_pci\_config_ops，也即写入这块内存空间，这就相当于对于这个 PCI 设备进行某种配置。

对 PCI 设备进行配置的时候，会有这样的调用链：virtio\_pci\_config\_write->virtio\_ioport\_write->virtio\_queue\_set\_addr。设置 virtio 的 queue 的地址是一项很重要的操作。

void 

 virtio\_queue\_set_addr(VirtIODevice *vdev, int n, hwaddr addr)

{

vdev->vq\[n\].vring.desc = addr;

virtio\_queue\_update_rings(vdev, n);

}

从这里我们可以看出，qemu 后端的 VirtIODevice 的 VirtQueue 的 vring 的地址，被设置成了刚才给队列分配的内存的 GPA。

![[2572f8b1e75b9eaab6560866fcb31fd0_cd3a4b1cf3714fde9.jpg]]

接着，我们来看一下这个队列的格式。

![[49414d5acc81933b66410bbba102b0db_d92ca61e628e4525b.jpg]]

struct 

 vring_desc {

__virtio64 addr;

__virtio32 len;

__virtio16 flags;

__virtio16 next;

};

struct 

 vring_avail {

__virtio16 flags;

__virtio16 idx;

__virtio16 ring\[\];

};

struct 

 vring\_used\_elem {

__virtio32 id;

__virtio32 len;

};

struct 

 vring_used {

__virtio16 flags;

__virtio16 idx;

struct 

 vring\_used\_elem ring\[\];

};

struct 

 vring {

unsigned 

 int num;

struct 

 vring_desc *desc;

struct 

 vring_avail *avail;

struct 

 vring_used *used;

};

vring 包含三个成员：

vring_desc 指向分配的内存块，用于存放客户机和 qemu 之间传输的数据。

avail->ring\[\]是发送端维护的环形队列，指向需要接收端处理的 vring_desc。

used->ring\[\]是接收端维护的环形队列，指向自己已经处理过了的 vring_desc。

## 数据写入的流程

接下来，我们来看，真的写入一个数据的时候，会发生什么。

按照上面 virtio 驱动初始化的时候的逻辑，blk\_mq\_make\_request 会被调用。这个函数比较复杂，会分成多个分支，但是最终都会调用到 request\_queue 的 virtio\_mq\_ops 的 queue_rq 函数。

struct 

 request_queue *q = rq->q;

q->mq_ops->queue_rq(hctx, &bd);

static 

 const 

 struct 

 blk\_mq\_ops virtio\_mq\_ops = {

.queue\_rq = virtio\_queue_rq,

.complete = virtblk\_request\_done,

.init\_request = virtblk\_init_request,

.map\_queues = virtblk\_map_queues,

};

根据 virtio\_mq\_ops 的定义，我们现在要调用 virtio\_queue\_rq。

static blk\_status\_t virtio\_queue\_rq(struct 

 blk\_mq\_hw_ctx *hctx,

const 

 struct 

 blk\_mq\_queue_data *bd)

{

struct 

 virtio_blk *vblk = hctx->queue->queuedata;

struct 

 request *req = bd->rq;

struct 

 virtblk_req *vbr = blk\_mq\_rq\_to\_pdu(req);

......

err = virtblk\_add\_req(vblk->vqs\[qid\].vq, vbr, vbr->sg, num);

......

if (notify)

virtqueue_notify(vblk->vqs\[qid\].vq);

return BLK\_STS\_OK;

}

在 virtio\_queue\_rq 中，我们会将请求写入的数据，通过 virtblk\_add\_req 放入 struct virtqueue。

因此，接下来的调用链为：virtblk\_add\_req->virtqueue\_add\_sgs->virtqueue_add。

static inline int virtqueue_add(struct 

 virtqueue *_vq,

struct 

 scatterlist *sgs\[\],

unsigned int total_sg,

unsigned int out_sgs,

unsigned int in_sgs,

void *data,

void *ctx,

gfp_t gfp)

{

struct 

 vring_virtqueue *vq = to_vvq(_vq);

struct 

 scatterlist *sg;

struct 

 vring_desc *desc;

unsigned int i, n, avail, descs_used, uninitialized_var(prev), err_idx;

int head;

bool indirect;

......

head = vq->free_head;

indirect = false;

desc = vq->vring.desc;

i = head;

descs\_used = total\_sg;

for (n = 0; n < out_sgs; n++) {

for (sg = sgs\[n\]; sg; sg = sg_next(sg)) {

dma\_addr\_t addr = vring\_map\_one_sg(vq, sg, DMA\_TO\_DEVICE);

......

desc\[i\].flags = cpu\_to\_virtio16(_vq->vdev, VRING\_DESC\_F_NEXT);

desc\[i\].addr = cpu\_to\_virtio64(_vq->vdev, addr);

desc\[i\].len = cpu\_to\_virtio32(_vq->vdev, sg->length);

prev = i;

i = virtio16\_to\_cpu(_vq->vdev, desc\[i\].next);

}

}

desc\[prev\].flags &= cpu\_to\_virtio16(_vq->vdev, ~VRING\_DESC\_F_NEXT);

vq->vq.num\_free -= descs\_used;

vq->free_head = i;

vq->desc_state\[head\].data = data;

avail = vq->avail\_idx\_shadow & (vq->vring.num - 1);

vq->vring.avail->ring\[avail\] = cpu\_to\_virtio16(_vq->vdev, head);

virtio_wmb(vq->weak_barriers);

vq->avail\_idx\_shadow++;

vq->vring.avail->idx = cpu\_to\_virtio16(_vq->vdev, vq->avail\_idx\_shadow);

vq->num_added++;

......

return 

 0;

}

在 virtqueue\_add 函数中，我们能看到，free\_head 指向的整个内存块空闲链表的起始位置，用 head 变量记住这个起始位置。

接下来，i 也指向这个起始位置，然后是一个 for 循环，将数据放到内存块里面，放的过程中，next 不断指向下一个空闲位置，这样空闲的内存块被不断的占用。等所有的写入都结束了，i 就会指向这次存放的内存块的下一个空闲位置，然后 free_head 就指向 i，因为前面的都填满了。

至此，从 head 到 i 之间的内存块，就是这次写入的全部数据。

于是，在 vring 的 avail 变量中，在 ring\[\]数组中分配新的一项，在 avail 的位置，avail 的计算是 avail\_idx\_shadow & (vq->vring.num - 1)，其中，avail\_idx\_shadow 是上一次的 avail 的位置。这里如果超过了 ring\[\]数组的下标，则重新跳到起始位置，就说明是一个环。这次分配的新的 avail 的位置就存放新写入的从 head 到 i 之间的内存块。然后是 avail\_idx\_shadow++，这说明这一块内存可以被接收方读取了。

接下来，我们回到 virtio\_queue\_rq，调用 virtqueue\_notify 通知接收方。而 virtqueue\_notify 会调用 vp_notify。

bool 

 vp_notify(struct 

 virtqueue *vq)

{

\* signal the other end */

iowrite16(vq->index, (void __iomem *)vq->priv);

return 

 true;

}

然后，我们写入一个 I/O 会触发 VM exit。我们在解析 CPU 的时候看到过这个逻辑。

int kvm\_cpu\_exec(CPUState *cpu)

{

struct 

 kvm_run *run = cpu->kvm_run;

int ret, run_ret;

......

run_ret = kvm\_vcpu\_ioctl(cpu, KVM_RUN, 0);

......

switch (run->exit_reason) {

case KVM\_EXIT\_IO:

DPRINTF("handle_io\\n");

kvm\_handle\_io(run->io.port, attrs,

(uint8_t *)run + run->io.data_offset,

run->io.direction,

run->io.size,

run->io.count);

ret = 0;

break;

}

......

}

这次写入的也是一个 I/O 的内存空间，同样会触发 virtio\_ioport\_write，这次会调用 virtio\_queue\_notify。

void virtio\_queue\_notify(VirtIODevice *vdev, int n)

{

VirtQueue *vq = &vdev->vq\[n\];

......

if (vq->handle\_aio\_output) {

event\_notifier\_set(&vq->host_notifier);

} else 

 if (vq->handle_output) {

vq->handle_output(vdev, vq);

}

}

virtio\_queue\_notify 会调用 VirtQueue 的 handle\_output 函数，前面我们已经设置过这个函数了，是 virtio\_blk\_handle\_output。

接下来的调用链为：virtio\_blk\_handle\_output->virtio\_blk\_handle\_output\_do->virtio\_blk\_handle\_vq。

bool virtio\_blk\_handle_vq(VirtIOBlock \*s, VirtQueue \*vq)

{

VirtIOBlockReq *req;

MultiReqBuffer 

 mrb 

 = {};

bool 

 progress 

 = 

 false;

......

do {

virtio\_queue\_set_notification(vq, 0);

while ((req = virtio\_blk\_get_request(s, vq))) {

progress = true;

if (virtio\_blk\_handle_request(req, &mrb)) {

virtqueue\_detach\_element(req->vq, &req->elem, 0);

virtio\_blk\_free_request(req);

break;

}

}

virtio\_queue\_set_notification(vq, 1);

} while (!virtio\_queue\_empty(vq));

if (mrb.num_reqs) {

virtio\_blk\_submit_multireq(s->blk, &mrb);

}

......

return progress;

}

在 virtio\_blk\_handle\_vq 中，有一个 while 循环，在循环中调用函数 virtio\_blk\_get\_request 从 vq 中取出请求，然后调用 virtio\_blk\_handle_request 处理从 vq 中取出的请求。

我们先来看 virtio\_blk\_get_request。

static VirtIOBlockReq *virtio\_blk\_get_request(VirtIOBlock \*s, VirtQueue \*vq)

{

VirtIOBlockReq *req = virtqueue_pop(vq, sizeof(VirtIOBlockReq));

if (req) {

virtio\_blk\_init_request(s, vq, req);

}

return req;

}

void *virtqueue_pop(VirtQueue *vq, size_t sz)

{

unsigned 

 int i, head, max;

VRingMemoryRegionCaches *caches;

MemoryRegionCache *desc_cache;

int64_t len;

VirtIODevice *vdev = vq->vdev;

VirtQueueElement *elem = NULL;

unsigned out\_num, in\_num, elem_entries;

hwaddr addr\[VIRTQUEUE\_MAX\_SIZE\];

struct 

 iovec iov\[VIRTQUEUE\_MAX\_SIZE\];

VRingDesc desc;

int rc;

......

out\_num = in\_num = elem_entries = 0;

max = vq->vring.num;

i = head;

caches = vring\_get\_region_caches(vq);

desc_cache = &caches->desc;

vring\_desc\_read(vdev, &desc, desc_cache, i);

......

do {

bool map_ok;

if (desc.flags & VRING\_DESC\_F_WRITE) {

map_ok = virtqueue\_map\_desc(vdev, &in\_num, addr + out\_num,

iov + out_num,

VIRTQUEUE\_MAX\_SIZE - out_num, true,

desc.addr, desc.len);

} else {

map_ok = virtqueue\_map\_desc(vdev, &out_num, addr, iov,

VIRTQUEUE\_MAX\_SIZE, false,

desc.addr, desc.len);

}

......

rc = virtqueue\_read\_next_desc(vdev, &desc, desc_cache, max, &i);

} while (rc == VIRTQUEUE\_READ\_DESC_MORE);

......

elem = virtqueue\_alloc\_element(sz, out\_num, in\_num);

elem->index = head;

for (i = 0; i < out_num; i++) {

elem->out_addr\[i\] = addr\[i\];

elem->out_sg\[i\] = iov\[i\];

}

for (i = 0; i < in_num; i++) {

elem->in\_addr\[i\] = addr\[out\_num + i\];

elem->in\_sg\[i\] = iov\[out\_num + i\];

}

vq->inuse++;

......

return elem;

}

我们可以看到，virtio\_blk\_get\_request 会调用 virtqueue\_pop。在这里面，我们能看到对于 vring 的操作，也即从这里面将客户机里面写入的数据读取出来，放到 VirtIOBlockReq 结构中。

接下来，我们就要调用 virtio\_blk\_handle\_request 处理这些数据。所以接下来的调用链为：virtio\_blk\_handle\_request->virtio\_blk\_submit\_multireq->submit\_requests。

static inline void submit_requests(BlockBackend \*blk, MultiReqBuffer \*mrb,int start, int num_reqs, int niov)

{

QEMUIOVector *qiov = &mrb->reqs\[start\]->qiov;

int64\_t sector\_num = mrb->reqs\[start\]->sector_num;

bool is_write = mrb->is_write;

if (num_reqs > 1) {

int i;

struct 

 iovec *tmp_iov = qiov->iov;

int tmp_niov = qiov->niov;

qemu\_iovec\_init(qiov, niov);

for (i = 0; i < tmp_niov; i++) {

qemu\_iovec\_add(qiov, tmp\_iov\[i\].iov\_base, tmp\_iov\[i\].iov\_len);

}

for (i = start + 1; i < start + num_reqs; i++) {

qemu\_iovec\_concat(qiov, &mrb->reqs\[i\]->qiov, 0,

mrb->reqs\[i\]->qiov.size);

mrb->reqs\[i - 1\]->mr_next = mrb->reqs\[i\];

}

block\_acct\_merge_done(blk\_get\_stats(blk),

is\_write ? BLOCK\_ACCT\_WRITE : BLOCK\_ACCT_READ,

num_reqs - 1);

}

if (is_write) {

blk\_aio\_pwritev(blk, sector\_num << BDRV\_SECTOR_BITS, qiov, 0,

virtio\_blk\_rw_complete, mrb->reqs\[start\]);

} else {

blk\_aio\_preadv(blk, sector\_num << BDRV\_SECTOR_BITS, qiov, 0,

virtio\_blk\_rw_complete, mrb->reqs\[start\]);

}

}

在 submit\_requests 中，我们看到了 BlockBackend。这是在 qemu 启动的时候，打开 qcow2 文件的时候生成的，现在我们可以用它来写入文件了，调用的是 blk\_aio_pwritev。

BlockAIOCB *blk\_aio\_pwritev(BlockBackend *blk, int64_t offset,

QEMUIOVector *qiov, BdrvRequestFlags flags,

BlockCompletionFunc *cb, void *opaque)

{

return 

 blk\_aio\_prwv(blk, offset, qiov->size, qiov,

blk\_aio\_write_entry, flags, cb, opaque);

}

static BlockAIOCB *blk\_aio\_prwv(BlockBackend *blk, int64_t offset, int bytes,

void *iobuf, CoroutineEntry co_entry,

BdrvRequestFlags flags,

BlockCompletionFunc *cb, void *opaque)

{

BlkAioEmAIOCB *acb;

Coroutine *co;

acb = blk\_aio\_get(&blk\_aio\_em\_aiocb\_info, blk, cb, opaque);

acb->rwco = (BlkRwCo) {

.blk = blk,

.offset = offset,

.iobuf = iobuf,

.flags = flags,

.ret = NOT_DONE,

};

acb->bytes = bytes;

acb->has_returned = false;

co = qemu\_coroutine\_create(co_entry, acb);

bdrv\_coroutine\_enter(blk_bs(blk), co);

acb->has_returned = true;

return &acb->common;

}

在 blk\_aio\_pwritev 中，我们看到，又是创建了一个协程来进行写入。写入完毕之后调用 virtio\_blk\_rw\_complete->virtio\_blk\_req\_complete。

static void virtio\_blk\_req_complete(VirtIOBlockReq *req, unsigned char status)

{

VirtIOBlock *s = req->dev;

VirtIODevice *vdev = VIRTIO_DEVICE(s);

trace\_virtio\_blk\_req\_complete(vdev, req, status);

stb_p(&req->in->status, status);

virtqueue_push(req->vq, &req->elem, req->in_len);

virtio_notify(vdev, req->vq);

}

在 virtio\_blk\_req\_complete 中，我们先是调用 virtqueue\_push，更新 vring 中 used 变量，表示这部分已经写入完毕，空间可以回收利用了。但是，这部分的改变仅仅改变了 qemu 后端的 vring，我们还需要通知客户机中 virtio 前端的 vring 的值，因而要调用 virtio\_notify。virtio\_notify 会调用 virtio_irq 发送一个中断。

还记得咱们前面注册过一个中断处理函数 vp_interrupt 吗？它就是干这个事情的。

static 

 irqreturn_t 

 vp_interrupt(int irq, void *opaque)

{

struct 

 virtio\_pci\_device *vp_dev = opaque;

u8 isr;

\* important to save off the value. */

isr = ioread8(vp_dev->isr);

if (isr & VIRTIO\_PCI\_ISR_CONFIG)

vp\_config\_changed(irq, opaque);

return 

 vp\_vring\_interrupt(irq, opaque);

}

就像前面说的一样 vp\_interrupt 这个中断处理函数，一是处理配置变化，二是处理 I/O 结束。第二种的调用链为：vp\_interrupt->vp\_vring\_interrupt->vring_interrupt。

irqreturn_t 

 vring_interrupt(int irq, void *_vq)

{

struct 

 vring_virtqueue *vq = to_vvq(_vq);

......

if (vq->vq.callback)

vq->vq.callback(&vq->vq);

return IRQ_HANDLED;

}

在 vring\_interrupt 中，我们会调用 callback 函数，这个也是在前面注册过的，是 virtblk\_done。

接下来的调用链为：virtblk\_done->virtqueue\_get\_buf->virtqueue\_get\_buf\_ctx。

void *virtqueue\_get\_buf_ctx(struct 

 virtqueue \*_vq, unsigned int \*len,

void **ctx)

{

struct 

 vring_virtqueue *vq = to_vvq(_vq);

void *ret;

unsigned int i;

u16 last_used;

......

last_used = (vq->last\_used\_idx & (vq->vring.num - 1));

i = virtio32\_to\_cpu(_vq->vdev, vq->vring.used->ring\[last_used\].id);

*len = virtio32\_to\_cpu(_vq->vdev, vq->vring.used->ring\[last_used\].len);

......

ret = vq->desc_state\[i\].data;

detach_buf(vq, i, ctx);

vq->last\_used\_idx++;

......

return ret;

}

在 virtqueue\_get\_buf\_ctx 中，我们可以看到，virtio 前端的 vring 中的 last\_used\_idx 加一，说明这块数据 qemu 后端已经消费完毕。我们可以通过 detach\_buf 将其放入空闲队列中，留给以后的写入请求使用。

至此，整个存储虚拟化的写入流程才全部完成。

## 总结时刻

下面我们来总结一下存储虚拟化的场景下，整个写入的过程。

在虚拟机里面，应用层调用 write 系统调用写入文件。

write 系统调用进入虚拟机里面的内核，经过 VFS，通用块设备层，I/O 调度层，到达块设备驱动。

虚拟机里面的块设备驱动是 virtio\_blk，它和通用的块设备驱动一样，有一个 request queue，另外有一个函数 make\_request\_fn 会被设置为 blk\_mq\_make\_request，这个函数用于将请求放入队列。

虚拟机里面的块设备驱动是 virtio\_blk 会注册一个中断处理函数 vp\_interrupt。当 qemu 写入完成之后，它会通知虚拟机里面的块设备驱动。

blk\_mq\_make\_request 最终调用 virtqueue\_add，将请求添加到传输队列 virtqueue 中，然后调用 virtqueue_notify 通知 qemu。

在 qemu 中，本来虚拟机正处于 KVM_RUN 的状态，也即处于客户机状态。

qemu 收到通知后，通过 VM exit 指令退出客户机状态，进入宿主机状态，根据退出原因，得知有 I/O 需要处理。

qemu 调用 virtio\_blk\_handle\_output，最终调用 virtio\_blk\_handle\_vq。

virtio\_blk\_handle\_vq 里面有一个循环，在循环中，virtio\_blk\_get\_request 函数从传输队列中拿出请求，然后调用 virtio\_blk\_handle_request 处理请求。

virtio\_blk\_handle\_request 会调用 blk\_aio_pwritev，通过 BlockBackend 驱动写入 qcow2 文件。

写入完毕之后，virtio\_blk\_req\_complete 会调用 virtio\_notify 通知虚拟机里面的驱动。数据写入完成，刚才注册的中断处理函数 vp_interrupt 会收到这个通知。

![[79ad143a3149ea36bc80219940d7d00c_1debe7fc5b5940ca9.jpg]]

## 课堂练习

请你沿着代码，仔细分析并牢记 virtqueue 的结构以及写入和读取方式。这个结构在下面的网络传输过程中，还要起大作用。

欢迎留言和我分享你的疑惑和见解，也欢迎收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。

![[8c0a95fa07a8b9a1abfd394479bdd637_86ca5b1728be49048.jpg]]